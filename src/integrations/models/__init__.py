"""
Project-S Modell Menedzser
-------------------------
Ez a modul kezeli az AI modell kivÃ¡lasztÃ¡sÃ¡t, vÃ¡ltÃ¡st Ã©s a modellek teljesÃ­tmÃ©nyÃ©nek nyomon kÃ¶vetÃ©sÃ©t.
"""

import os
import logging
import asyncio
import json
from typing import Dict, Any, List, Optional, Union, Tuple
import time
from datetime import datetime
from pathlib import Path

from integrations.multi_model_ai_client import multi_model_ai_client
from integrations.persistent_state_manager import persistent_state_manager
from integrations.core_execution_bridge import core_execution_bridge

# Tool integration for actual task execution
logger = logging.getLogger(__name__)

try:
    from tools.tool_registry import tool_registry
    TOOLS_AVAILABLE = True
    logger.info("Tool registry imported successfully - tools available for execution")
except ImportError as e:
    TOOLS_AVAILABLE = False
    logger.warning(f"Tool registry not available: {e}")

# Intelligent workflow integration
try:
    from integrations.intelligent_workflow_integration import (
        intelligent_workflow_orchestrator,
        process_with_intelligent_workflow
    )
    INTELLIGENT_WORKFLOWS_AVAILABLE = True
    logger.info("âœ… Intelligent workflow integration imported successfully")
except ImportError as e:
    INTELLIGENT_WORKFLOWS_AVAILABLE = False
    logger.warning(f"âš ï¸ Intelligent workflow integration not available: {e}")

class ModelManager:
    """
    Modell menedzser osztÃ¡ly a modellek kezelÃ©sÃ©hez Ã©s intelligens modellvÃ¡lasztÃ¡shoz.
    """
    
    def __init__(self):
        """Modell menedzser inicializÃ¡lÃ¡sa."""
        self.ai_client = multi_model_ai_client
        
        # Modell metrikÃ¡k
        self.model_metrics = {}
          # AlapÃ©rtelmezett modell (config-bÃ³l betÃ¶ltve)
        self.default_model = self._get_default_model_from_config()
        
        # Modell hasznÃ¡lat kÃ¶vetÃ©se
        self.model_usage = {}
        
        # UtolsÃ³ modellvÃ¡ltÃ¡s ideje
        self.last_model_switch_time = None
        
        # Modell teljesÃ­tmÃ©ny cache
        self.performance_cache_path = Path(__file__).parent.parent / "memory" / "model_performance_cache.json"
        self.performance_cache = self._load_performance_cache()
        
        # HozzÃ¡fÃ©rÃ©s a perzisztens Ã¡llapotkezelÅ‘hÃ¶z
        self.state_manager = persistent_state_manager
        
        logger.info("Modell menedzser inicializÃ¡lva")
        
    def _load_performance_cache(self) -> Dict[str, Any]:
        """BetÃ¶lti a modell teljesÃ­tmÃ©ny cache-t, ha lÃ©tezik."""
        try:
            if self.performance_cache_path.exists():
                with open(self.performance_cache_path, 'r', encoding='utf-8') as file:
                    return json.load(file)
            return {}
        except Exception as e:
            logger.error(f"Hiba a teljesÃ­tmÃ©ny cache betÃ¶ltÃ©se kÃ¶zben: {e}")
            return {}
    
    def _save_performance_cache(self) -> None:
        """Elmenti a modell teljesÃ­tmÃ©ny cache-t."""
        try:
            # BiztosÃ­tjuk, hogy a kÃ¶nyvtÃ¡r lÃ©tezik
            self.performance_cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.performance_cache_path, 'w', encoding='utf-8') as file:
                json.dump(self.performance_cache, file, indent=2)
                
        except Exception as e:
            logger.error(f"Hiba a teljesÃ­tmÃ©ny cache mentÃ©se kÃ¶zben: {e}")
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """
        Visszaadja az elÃ©rhetÅ‘ modellek listÃ¡jÃ¡t.
        
        Returns:
            List[Dict]: Az elÃ©rhetÅ‘ modellek listÃ¡ja
        """
        return self.ai_client.list_available_models()
        
    def determine_task_type(self, query: str) -> str:
        """
        MeghatÃ¡rozza a bemeneti lekÃ©rdezÃ©s tÃ­pusÃ¡t.
        
        Args:
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            
        Returns:
            str: A feladat tÃ­pusa (pl. "tervezÃ©s", "kÃ³dolÃ¡s", stb.)
        """
        # Itt kÃ©sÅ‘bb komplett feladat tÃ­pus detektÃ¡lÃ³ logikÃ¡t lehet implementÃ¡lni
          # EgyszerÅ± kulcsszÃ³ alapÃº detektÃ¡lÃ¡s (magyar Ã©s angol tÃ¡mogatÃ¡ssal)
        keywords = {
            "tervezÃ©s": ["tervezÃ©s", "tervezzÃ¼nk", "tervezd meg", "Ã©pÃ­tsd fel", "architektÃºra", "planning", "plan", "design", "architecture"],
            "kÃ³dolÃ¡s": ["kÃ³d", "programozÃ¡s", "implementÃ¡ciÃ³", "fejlesztÃ©s", "scriptet", "code", "coding", "programming", "development", "script"],
            "dokumentÃ¡ciÃ³": ["dokumentÃ¡ciÃ³", "magyarÃ¡zd el", "dokumentumd", "Ã­rj leÃ­rÃ¡st", "documentation", "document", "explain", "describe"],
            "adatelemzÃ©s": ["elemezd", "adatelemzÃ©s", "adatfeldolgozÃ¡s", "statisztika", "analysis", "analyze", "data analysis", "statistics"],
            "kreatÃ­v_Ã­rÃ¡s": ["kreatÃ­v", "tÃ¶rtÃ©net", "Ã­rj egy", "fogalmazÃ¡s", "creative", "story", "writing", "compose"],
            "fordÃ­tÃ¡s": ["fordÃ­tsd", "fordÃ­tÃ¡s", "angolul", "magyarul", "translate", "translation"],
            "Ã¶sszefoglalÃ¡s": ["Ã¶sszefoglalÃ¡s", "Ã¶sszegzÃ©s", "rÃ¶viden foglald", "tÃ¶mÃ¶rÃ­ts", "summary", "summarize", "brief"],
            "gyors_vÃ¡lasz": ["gyors", "rÃ¶vid", "gyorsan", "quick", "fast", "short", "briefly"]
        }
        
        # Legjobb egyezÃ©s keresÃ©se
        task_matches = {}
        query_lower = query.lower()
        
        for task_type, task_keywords in keywords.items():
            matches = sum(1 for keyword in task_keywords if keyword.lower() in query_lower)
            if matches > 0:
                task_matches[task_type] = matches
                
        if task_matches:
            # LegtÃ¶bb egyezÃ©s kivÃ¡lasztÃ¡sa
            best_task = max(task_matches.items(), key=lambda x: x[1])[0]
            logger.info(f"Feladat tÃ­pus meghatÃ¡rozva: {best_task}")
            return best_task
            
        # AlapÃ©rtelmezettkÃ©nt feltÃ©telezzÃ¼k, hogy gyors vÃ¡lasz
        logger.info("Nem ismert fel specifikus feladat tÃ­pust, 'gyors_vÃ¡lasz' hasznÃ¡lata")
        return "gyors_vÃ¡lasz"
    
    async def select_model_for_task(self, query: str, task_type: Optional[str] = None) -> str:
        """
        KivÃ¡laszt egy megfelelÅ‘ modellt a feladathoz.
        
        Args:
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            task_type: OpcionÃ¡lis explicit feladat tÃ­pus
            
        Returns:
            str: A kivÃ¡lasztott modell azonosÃ­tÃ³ja
        """
        # Ha nincs explicit feladat tÃ­pus, detektÃ¡ljuk
        if task_type is None:
            task_type = self.determine_task_type(query)
        # Fallback: ha nincs ilyen metÃ³dus, alapÃ©rtelmezett modell
        if hasattr(self.ai_client, 'suggest_model_for_task'):
            suggested_model = self.ai_client.suggest_model_for_task(task_type)
        else:
            suggested_model = self.default_model
        logger.info(f"A(z) '{task_type}' feladat tÃ­pushoz a(z) '{suggested_model}' modellt vÃ¡lasztottuk")
        return suggested_model
    
    def record_model_performance(self, 
                              model_id: str, 
                              task_type: str, 
                              response_time: float, 
                              success: bool = True) -> None:
        """
        RÃ¶gzÃ­ti egy modell teljesÃ­tmÃ©nyÃ©t.
        
        Args:
            model_id: A modell azonosÃ­tÃ³ja
            task_type: A feladat tÃ­pusa
            response_time: A vÃ¡laszidÅ‘ mÃ¡sodpercben
            success: Sikeres volt-e a kÃ©rÃ©s
        """
        # Ha a modell mÃ©g nem szerepel a teljesÃ­tmÃ©ny adatbÃ¡zisban, adjuk hozzÃ¡
        if model_id not in self.performance_cache:
            self.performance_cache[model_id] = {}
            
        # Ha az adott feladattÃ­pushoz mÃ©g nincs adat, inicializÃ¡ljuk
        if task_type not in self.performance_cache[model_id]:
            self.performance_cache[model_id][task_type] = {
                "request_count": 0,
                "success_count": 0,
                "failure_count": 0,
                "total_response_time": 0,
                "avg_response_time": 0,
                "last_used": datetime.now().isoformat()
            }
            
        # FrissÃ­tjÃ¼k a statisztikÃ¡kat
        stats = self.performance_cache[model_id][task_type]
        stats["request_count"] += 1
        if success:
            stats["success_count"] += 1
        else:
            stats["failure_count"] += 1
            
        stats["total_response_time"] += response_time
        stats["avg_response_time"] = stats["total_response_time"] / stats["request_count"]
        stats["last_used"] = datetime.now().isoformat()
        
        # ElmentjÃ¼k a frissÃ­tett cache-t
        self._save_performance_cache()
            
    async def execute_task_with_model(self, 
                                    query: str,
                                    system_message: Optional[str] = None,
                                    model: Optional[str] = None,
                                    task_type: Optional[str] = None,
                                    temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        VÃ©grehajt egy feladatot a legmegfelelÅ‘bb modellel vagy a megadott modellel.
        
        Args:
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            system_message: OpcionÃ¡lis rendszerÃ¼zenet
            model: Explicit modell azonosÃ­tÃ³, ha meg van adva
            task_type: Explicit feladat tÃ­pus, ha meg van adva 
            temperature: OpcionÃ¡lis hÅ‘mÃ©rsÃ©klet beÃ¡llÃ­tÃ¡s
            
        Returns:
            Dict: A vÃ¡lasz szÃ³tÃ¡r a tartalommal Ã©s metaadatokkal
        """
        # Ha nincs explicit feladat tÃ­pus, meghatÃ¡rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs explicit modell, kivÃ¡lasztjuk
        if model is None:
            model = await self.select_model_for_task(query, task_type)
              # MÃ©rjÃ¼k a vÃ¡laszidÅ‘t
        start_time = time.time()
        success = True
        
        try:
            # VÃ©grehajtjuk a feladatot
            result = await self.ai_client.generate_response(
                prompt=query,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            )
            
        except Exception as e:
            logger.error(f"Hiba a modell vÃ©grehajtÃ¡sa kÃ¶zben ({model}): {e}")
            success = False
            
            # VisszaesÃ©s az alapÃ©rtelmezett modellre
            logger.info(f"VisszaesÃ©s az alapÃ©rtelmezett modellre ({self.default_model})")
            try:
                result = await self.ai_client.generate_response(
                    prompt=query,
                    system_message=system_message,
                    model=self.default_model,
                    task_type=task_type,
                    temperature=temperature
                )
                model = self.default_model
            except Exception as fallback_e:
                logger.error(f"Hiba a visszaesÃ©si modell vÃ©grehajtÃ¡sa kÃ¶zben: {fallback_e}")
                result = None  # Explicit None meghatÃ¡rozÃ¡s
                model = self.default_model
        
        # KiszÃ¡moljuk a vÃ¡laszidÅ‘t
        end_time = time.time()
        response_time = end_time - start_time
        
        # RÃ¶gzÃ­tjÃ¼k a teljesÃ­tmÃ©nyt
        self.record_model_performance(model, task_type, response_time, success)
        
        # Ha a result None, string vagy dict, mindent kezelÃ¼nk
        if result is None:
            result = {
                "response": "No response received",
                "model": model,
                "task_type": task_type,
                "success": False,
                "response_time": response_time,
                "error": "Response was None"
            }
        elif isinstance(result, str):
            result = {
                "response": result,
                "model": model,
                "task_type": task_type,
                "success": success,
                "response_time": response_time
            }
        elif isinstance(result, dict):
            # HozzÃ¡adjuk a vÃ¡laszidÅ‘t a vÃ¡laszhoz
            result["response_time"] = response_time
            result["model"] = model
            result["task_type"] = task_type
        else:
            # EgyÃ©b tÃ­pusok esetÃ©n string reprezentÃ¡ciÃ³t hasznÃ¡lunk
            result = {
                "response": str(result),
                "model": model,
                "task_type": task_type,
                "success": success,
                "response_time": response_time
            }
        
        return result
    
    async def run_task_with_multiple_models(self, 
                                         query: str,
                                         system_message: Optional[str] = None,
                                         models: List[str] = [],
                                         task_type: Optional[str] = None,
                                         temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        VÃ©grehajt egy feladatot tÃ¶bb kÃ¼lÃ¶nbÃ¶zÅ‘ modellel.
        
        Args:
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            system_message: OpcionÃ¡lis rendszerÃ¼zenet
            models: A hasznÃ¡landÃ³ modellek listÃ¡ja
            task_type: Explicit feladat tÃ­pus, ha meg van adva
            temperature: OpcionÃ¡lis hÅ‘mÃ©rsÃ©klet beÃ¡llÃ­tÃ¡s
            
        Returns:
            Dict: A vÃ¡laszok szÃ³tÃ¡ra modell azonosÃ­tÃ³kkal
        """
        # Ha nincs explicit feladat tÃ­pus, meghatÃ¡rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs megadva modellek, akkor javaslatot kÃ©rÃ¼nk
        if not models:
            # Ha a feladathoz tartoznak ajÃ¡nlott modellek a konfigban, hasznÃ¡ljuk azokat
            task_models = self.ai_client.config.get("task_model_mapping", {}).get(task_type, [])
            if task_models:
                models = task_models[:2]  # ElsÅ‘ 2 ajÃ¡nlott modell hasznÃ¡lata 
            else:
                # KÃ¼lÃ¶nben hasznÃ¡ljuk az alapÃ©rtelmezettet Ã©s egy mÃ¡sikat
                models = [self.default_model, "claude-3-sonnet"]
                
        # PÃ¡rhuzamosan futtatjuk a lekÃ©rdezÃ©seket minden modellel
        tasks = []
        for model in models:
            tasks.append(self.execute_task_with_model(
                query=query,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            ))
            
        # VÃ¡rjuk meg az Ã¶sszes feladat befejezÃ©sÃ©t
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Feldolgozzuk az eredmÃ©nyeket
        model_responses = {}
        for model, result in zip(models, results):
            if isinstance(result, Exception):
                model_responses[model] = {
                    "error": True,
                    "message": str(result),
                    "model": model
                }
            else:
                model_responses[model] = result
                
        return {
            "task_type": task_type,
            "models_used": models,
            "responses": model_responses
        }
    
    async def execute_task_with_tools(self, 
                                     query: str,
                                     system_message: Optional[str] = None,
                                     model: Optional[str] = None,
                                     task_type: Optional[str] = None,
                                     temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        ðŸ”¥ KRITIKUS JAVÃTÃS: VÃ©grehajtja a feladatot AI tervezÃ©ssel Ã‰S valÃ³di tool vÃ©grehajtÃ¡ssal.
        
        Ez a metÃ³dus hidalja Ã¡t az AI vÃ¡laszok Ã©s a valÃ³di tool vÃ©grehajtÃ¡sok kÃ¶zÃ¶tti hÃ©zagot.
        
        Args:
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            system_message: OpcionÃ¡lis rendszerÃ¼zenet
            model: Explicit modell azonosÃ­tÃ³
            task_type: Explicit feladat tÃ­pus
            temperature: OpcionÃ¡lis hÅ‘mÃ©rsÃ©klet beÃ¡llÃ­tÃ¡s
            
        Returns:
            Dict: AI vÃ¡lasz + valÃ³di tool vÃ©grehajtÃ¡sok eredmÃ©nye
        """
        if not TOOLS_AVAILABLE:
            logger.warning("Tools not available, falling back to AI-only response")
            return await self.execute_task_with_model(query, system_message, model, task_type, temperature)
        
        # 1. FÃZIS: AI ElemzÃ©s Ã©s Tool TervezÃ©s
        planning_prompt = f"""
        Elemezd ezt a feladatot Ã©s hatÃ¡rozd meg, milyen konkrÃ©t tools-okat kell hasznÃ¡lni:
        
        FELADAT: {query}
        
        ElÃ©rhetÅ‘ tools: {list(tool_registry.tool_classes.keys()) if tool_registry else []}
        
        VÃ¡laszolj ebben a formÃ¡tumban:
        ELEMZÃ‰S: [rÃ¶vid elemzÃ©s]
        TOOLS_NEEDED: [tool1, tool2, tool3]
        PARAMETERS: 
        - tool1: {{param1: value1, param2: value2}}
        - tool2: {{param1: value1}}
        EXECUTION_ORDER: [tool1, tool2, tool3]
        
        Csak azokat a tools-okat hasznÃ¡ld, amik a listÃ¡ban vannak!
        """
        
        planning_result = await self.execute_task_with_model(
            query=planning_prompt,
            system_message="Te egy task elemzÅ‘ Ã©s tool planner vagy. LegyÃ©l specifikus a tool paramÃ©terekkel.",
            model=model,
            task_type="planning"
        )
        
        # 2. FÃZIS: Tool ParamÃ©terek KinyerÃ©se
        ai_response = planning_result.get("content", "")
        tools_executed = []
        
        try:
            # BetÃ¶ltjÃ¼k a tools-okat
            await tool_registry.load_tools()
            
            # EgyszerÅ± parsing a tool nevekhez (ezt kÃ©sÅ‘bb lehet javÃ­tani)
            lines = ai_response.split('\n')
            tools_to_execute = []
            
            for line in lines:
                if line.startswith('TOOLS_NEEDED:'):
                    tools_part = line.replace('TOOLS_NEEDED:', '').strip()
                    # KivesszÃ¼k a tool neveket
                    tools_part = tools_part.replace('[', '').replace(']', '')
                    tool_names = [t.strip() for t in tools_part.split(',') if t.strip()]
                    tools_to_execute = tool_names
                    break
            
            logger.info(f"Tools to execute: {tools_to_execute}")
            
            # 3. FÃZIS: ValÃ³di Tool VÃ©grehajtÃ¡s
            for tool_name in tools_to_execute:
                tool_name_clean = tool_name.strip()
                
                if tool_name_clean in tool_registry.tool_classes:
                    try:
                        tool_instance = tool_registry.get_tool(tool_name_clean)
                          # EgyszerÅ± paramÃ©ter generÃ¡lÃ¡s a feladat alapjÃ¡n
                        if "FileWriteTool" in tool_name_clean and ("fÃ¡jl" in query.lower() or "file" in query.lower() or "Ã­rj" in query.lower()):
                            # FÃ¡jl Ã­rÃ¡s - intelligens fÃ¡jlnÃ©v kinyerÃ©s
                            filename = self._extract_filename_from_query(query)
                            content = f"Project-S vÃ©grehajtÃ¡s eredmÃ©nye:\n{query}\n\nLÃ©trehozva: {datetime.now()}"
                            
                            result = await tool_instance.execute(path=filename, content=content)
                            tools_executed.append({
                                "tool": tool_name_clean,
                                "parameters": {"path": filename, "content": content[:50] + "..."},
                                "result": result,
                                "success": result.get("success", False)
                            })
                            
                        elif "FileReadTool" in tool_name_clean:
                            # PrÃ³bÃ¡ljunk olvasni egy lÃ©tezÅ‘ fÃ¡jlt
                            import os
                            for filename in ["project_s_output.txt", "README.md", "requirements.txt"]:
                                if os.path.exists(filename):
                                    result = await tool_instance.execute(path=filename)
                                    tools_executed.append({
                                        "tool": tool_name_clean,
                                        "parameters": {"path": filename},
                                        "result": result,
                                        "success": result.get("success", False)
                                    })
                                    break
                                    
                        elif "SystemCommandTool" in tool_name_clean:
                            # BiztonsÃ¡gos parancs vÃ©grehajtÃ¡s
                            safe_commands = {
                                "lista": "dir",
                                "list": "dir", 
                                "files": "dir",
                                "hello": "echo Hello from Project-S",
                                "test": "echo Tool execution working!"
                            }
                            
                            command = "echo Tool execution successful!"
                            for keyword, cmd in safe_commands.items():
                                if keyword in query.lower():
                                    command = cmd
                                    break
                            
                            result = await tool_instance.execute(command=command)
                            tools_executed.append({
                                "tool": tool_name_clean,
                                "parameters": {"command": command},
                                "result": result,
                                "success": result.get("success", False)
                            })
                            
                        else:
                            logger.info(f"Tool {tool_name_clean} requires specific parameter handling")
                            
                    except Exception as e:
                        logger.error(f"Error executing tool {tool_name_clean}: {e}")
                        tools_executed.append({
                            "tool": tool_name_clean,
                            "error": str(e),
                            "success": False
                        })
                else:
                    logger.warning(f"Tool {tool_name_clean} not found in registry")
            
        except Exception as e:
            logger.error(f"Error in tool execution phase: {e}")
        
        # 4. FÃZIS: EredmÃ©ny Ã–sszegzÃ©s
        summary_prompt = f"""
        Eredeti feladat: {query}
        
        AI tervezÃ©s: {ai_response[:200]}...
        
        VÃ©grehajtott tools: {len(tools_executed)}
        Tool eredmÃ©nyek: {[t.get('success', False) for t in tools_executed]}
        
        KÃ©szÃ­ts egy rÃ¶vid Ã¶sszegzÃ©st: mi tÃ¶rtÃ©nt valÃ³jÃ¡ban, milyen fÃ¡jlok/mÅ±veletek jÃ¶ttek lÃ©tre.
        """
        
        summary_result = await self.execute_task_with_model(
            query=summary_prompt,
            system_message="KÃ©szÃ­ts rÃ¶vid, faktikus Ã¶sszegzÃ©st a vÃ©grehajtott mÅ±veletekrÅ‘l.",
            model=model,
            task_type="summary"
        )
        
        # Teljes eredmÃ©ny visszaadÃ¡sa
        return {
            "ai_planning": planning_result,
            "tools_executed": tools_executed,
            "tools_count": len(tools_executed),
            "tools_successful": len([t for t in tools_executed if t.get('success', False)]),
            "summary": summary_result,
            "execution_type": "AI_PLUS_TOOLS",  # Jelzi, hogy ez valÃ³di vÃ©grehajtÃ¡s volt
            "response_time": planning_result.get("response_time", 0)
        }
    
    async def get_model_performance_stats(self, model_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Visszaadja a modell(ek) teljesÃ­tmÃ©ny statisztikÃ¡it.
        
        Args:
            model_id: OpcionÃ¡lis modell azonosÃ­tÃ³, ha csak egy modellre vagyunk kÃ­vÃ¡ncsiak
            
        Returns:
            Dict: A modellek teljesÃ­tmÃ©ny statisztikÃ¡i
        """
        if model_id:
            # Ha csak egy modellet kÃ©rÃ¼nk
            if model_id in self.performance_cache:
                return {model_id: self.performance_cache[model_id]}
            return {model_id: "Nincs teljesÃ­tmÃ©ny adat a modellhez"}
            
        # Minden modellhez visszaadjuk a statisztikÃ¡kat
        return self.performance_cache

    async def execute_task_with_model_in_session(self, 
                                         session_id: str,
                                         query: str,
                                         system_message: Optional[str] = None,
                                         model: Optional[str] = None,
                                         task_type: Optional[str] = None,
                                         temperature: Optional[float] = None,
                                         persist_history: bool = True) -> Dict[str, Any]:
        """
        VÃ©grehajtja a feladatot egy adott modellel, egy adott munkamenetben,
        opcionÃ¡lisan mentve a beszÃ©lgetÃ©s tÃ¶rtÃ©netÃ©t.
        
        Args:
            session_id: A munkamenet azonosÃ­tÃ³ja
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            system_message: OpcionÃ¡lis rendszerÃ¼zenet
            model: OpcionÃ¡lis modell azonosÃ­tÃ³ (ha nincs megadva, kivÃ¡lasztÃ¡sra kerÃ¼l)
            task_type: OpcionÃ¡lis feladat tÃ­pus
            temperature: OpcionÃ¡lis hÅ‘mÃ©rsÃ©klet Ã©rtÃ©k
            persist_history: Ha True, a beszÃ©lgetÃ©s bekerÃ¼l a perzisztens tÃ¡rolÃ³ba
            
        Returns:
            Dict: A vÃ¡lasz szÃ³tÃ¡r a tartalommal Ã©s metaadatokkal
        """
        # Az elÅ‘zÅ‘ beszÃ©lgetÃ©sek betÃ¶ltÃ©se a munkamenetbÅ‘l
        conversation_history = []
        if persist_history:
            conversation_history = await self.state_manager.get_conversation_history(session_id)
        
        # Ha nincs explicit feladat tÃ­pus, meghatÃ¡rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs explicit modell, kivÃ¡lasztjuk
        if model is None:
            model = await self.select_model_for_task(query, task_type)
        
        # Ha van beszÃ©lgetÃ©si elÅ‘zmÃ©ny, hozzÃ¡adjuk a prompt elejÃ©hez
        enhanced_prompt = query
        if conversation_history:
            # Az utolsÃ³ nÃ©hÃ¡ny beszÃ©lgetÃ©s hozzÃ¡adÃ¡sa kontextuskÃ©nt
            recent_history = conversation_history[-5:]  # UtolsÃ³ 5 beszÃ©lgetÃ©s
            context = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in recent_history
            ])
            enhanced_prompt = f"ElÅ‘zmÃ©nyek:\n{context}\n\nAktuÃ¡lis kÃ©rdÃ©s:\n{query}"
            
        # MÃ©rjÃ¼k a vÃ¡laszidÅ‘t
        start_time = time.time()
        success = True
        
        try:
            # VÃ©grehajtjuk a feladatot
            result = await self.ai_client.generate_response(
                prompt=enhanced_prompt,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            )
            
        except Exception as e:
            logger.error(f"Hiba a modell vÃ©grehajtÃ¡sa kÃ¶zben ({model}): {e}")
            success = False
            
            # VisszaesÃ©s az alapÃ©rtelmezett modellre
            logger.info(f"VisszaesÃ©s az alapÃ©rtelmezett modellre ({self.default_model})")
            result = await self.ai_client.generate_response(
                prompt=enhanced_prompt,
                system_message=system_message,
                model=self.default_model,
                task_type=task_type,
                temperature=temperature
            )
            model = self.default_model
        
        # KiszÃ¡moljuk a vÃ¡laszidÅ‘t
        end_time = time.time()
        response_time = end_time - start_time
        
        # RÃ¶gzÃ­tjÃ¼k a teljesÃ­tmÃ©nyt
        self.record_model_performance(
            model_id=model,
            task_type=task_type,
            response_time=response_time,
            success=success
        )
          # A vÃ¡laszok mentÃ©se a perzisztens tÃ¡rolÃ³ba, ha szÃ¼ksÃ©ges
        if persist_history:
            # FelhasznÃ¡lÃ³i Ã¼zenet mentÃ©se
            await self.state_manager.add_conversation_entry(
                session_id=session_id, 
                role="user", 
                content=query,
                metadata={"task_type": task_type}
            )
            
            # Handle both string and dictionary responses
            if isinstance(result, str):
                content = result
                provider = self._get_provider_for_model(model)
            else:
                content = result.get("content", str(result)) if result else "No response"
                provider = result.get("provider") if result else None
            
            # Asszisztens vÃ¡laszÃ¡nak mentÃ©se
            await self.state_manager.add_conversation_entry(
                session_id=session_id, 
                role="assistant", 
                content=content,
                metadata={
                    "model": model,
                    "provider": provider,
                    "response_time": response_time
                }
            )
        
        # Visszaadjuk az eredmÃ©nyt a vÃ¡laszidÅ‘vel Ã©s egyÃ©b metaadatokkal
        if isinstance(result, str):
            return {
                "content": result,
                "model": model,
                "provider": self._get_provider_for_model(model),
                "response_time": response_time,
                "session_id": session_id
            }
        elif isinstance(result, dict):
            result.update({
                "response_time": response_time,
                "session_id": session_id
            })
            return result
        else:
            return {
                "content": str(result) if result else "No response",
                "model": model,
                "provider": self._get_provider_for_model(model),
                "response_time": response_time,
                "session_id": session_id
            }
    
    async def run_task_with_multiple_models_in_session(self, 
                                                    session_id: str,
                                                    query: str,
                                                    system_message: Optional[str] = None,
                                                    models: List[str] = [],
                                                    task_type: Optional[str] = None,
                                                    temperature: Optional[float] = None,
                                                    persist_history: bool = True) -> Dict[str, Any]:
        """
        VÃ©grehajtja a feladatot tÃ¶bb modellel egy adott munkamenetben,
        Ã©s Ã¶sszehasonlÃ­tja az eredmÃ©nyeiket.
        
        Args:
            session_id: A munkamenet azonosÃ­tÃ³ja
            query: A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            system_message: OpcionÃ¡lis rendszerÃ¼zenet
            models: A hasznÃ¡landÃ³ modellek listÃ¡ja. Ha Ã¼res, automatikusan kivÃ¡laszt nÃ©hÃ¡nyat.
            task_type: OpcionÃ¡lis feladat tÃ­pus
            temperature: OpcionÃ¡lis hÅ‘mÃ©rsÃ©klet Ã©rtÃ©k
            persist_history: Ha True, a beszÃ©lgetÃ©s bekerÃ¼l a perzisztens tÃ¡rolÃ³ba
            
        Returns:
            Dict: SzÃ³tÃ¡r a kÃ¼lÃ¶nbÃ¶zÅ‘ modellek vÃ¡laszaival
        """
        # Az elÅ‘zÅ‘ beszÃ©lgetÃ©sek betÃ¶ltÃ©se a munkamenetbÅ‘l
        conversation_history = []
        if persist_history:
            conversation_history = await self.state_manager.get_conversation_history(session_id)
            
        # Ha nincs explicit feladat tÃ­pus, meghatÃ¡rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs megadva modell, akkor a feladattÃ­pusra ajÃ¡nlott modelleket hasznÃ¡ljuk
        if not models:
            task_models = self.ai_client.config.get("task_model_mapping", {}).get(task_type, [])
            if task_models:
                models = task_models[:2]  # ElsÅ‘ 2 ajÃ¡nlott modell hasznÃ¡lata 
            else:
                # KÃ¼lÃ¶nben hasznÃ¡ljuk az alapÃ©rtelmezettet Ã©s egy mÃ¡sikat
                models = [self.default_model, "claude-3-sonnet"]
        
        # Ha van beszÃ©lgetÃ©si elÅ‘zmÃ©ny, hozzÃ¡adjuk a prompt elejÃ©hez
        enhanced_prompt = query
        if conversation_history:
            # Az utolsÃ³ nÃ©hÃ¡ny beszÃ©lgetÃ©s hozzÃ¡adÃ¡sa kontextuskÃ©nt
            recent_history = conversation_history[-5:]  # UtolsÃ³ 5 beszÃ©lgetÃ©s
            context = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in recent_history
            ])
            enhanced_prompt = f"ElÅ‘zmÃ©nyek:\n{context}\n\nAktuÃ¡lis kÃ©rdÃ©s:\n{query}"
                
        # PÃ¡rhuzamosan futtatjuk a lekÃ©rdezÃ©seket minden modellel
        tasks = []
        for model in models:
            task = self.ai_client.generate_response(
                prompt=enhanced_prompt,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            )
            tasks.append(task)
            
        # VÃ¡rjuk meg az Ã¶sszes eredmÃ©nyt
        results = await asyncio.gather(*tasks, return_exceptions=True)
          # Feldolgozzuk az eredmÃ©nyeket
        model_responses = {}
        for i, result in enumerate(results):
            model = models[i]
            
            # EllenÅ‘rizzÃ¼k, hogy sikeres volt-e a vÃ©grehajtÃ¡s
            if isinstance(result, Exception):
                model_responses[model] = {
                    "error": True,
                    "message": str(result),
                    "model": model
                }
            elif result is None:
                model_responses[model] = {
                    "error": True,
                    "message": "No response received",
                    "model": model
                }
            else:
                # Ha a vÃ¡lasz string, akkor csomagoljuk be dictionary-ba
                if isinstance(result, str):
                    model_responses[model] = {
                        "content": result,
                        "model": model,
                        "success": True
                    }
                else:
                    model_responses[model] = result
                    
                    # RÃ¶gzÃ­tjÃ¼k a teljesÃ­tmÃ©nyt sikeres futtatÃ¡snÃ¡l
                    if isinstance(result, dict) and "response_time" in result:
                        self.record_model_performance(
                            model_id=model,
                            task_type=task_type,
                            response_time=result["response_time"],
                            success=True
                        )
                
        # ElmentjÃ¼k a felhasznÃ¡lÃ³i kÃ©rdÃ©st, ha szÃ¼ksÃ©ges
        if persist_history:
            await self.state_manager.add_conversation_entry(
                session_id=session_id, 
                role="user", 
                content=query,
                metadata={"task_type": task_type}
            )
            
            # ElmentjÃ¼k az Ã¶sszes asszisztens vÃ¡laszt
            for model, response in model_responses.items():
                if not isinstance(response, dict) or "error" in response:
                    continue
                    
                await self.state_manager.add_conversation_entry(
                    session_id=session_id, 
                    role="assistant", 
                    content=response.get("content", ""),                    metadata={
                        "model": model,
                        "provider": response.get("provider"),
                        "multi_model_comparison": True,
                        "response_time": response.get("response_time")
                    }
                )
        
        return {
            "task_type": task_type,
            "models_used": models,
            "responses": model_responses,
            "session_id": session_id
        }
    
    def _get_provider_for_model(self, model_id: str) -> Optional[str]:
        """Get the provider for a specific model."""
        return self.ai_client._get_provider_for_model(model_id)

    async def execute_task_with_core_system(self,
                                           query: str,
                                           system_message: Optional[str] = None,
                                           model: Optional[str] = None,
                                           task_type: Optional[str] = None,
                                           temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        ðŸ”¥ KRITIKUS JAVÃTÃS: VÃ©grehajtja a feladatot az eredeti core_old rendszerrel.
        Ez a metÃ³dus hasznÃ¡lja a core_old execution bridge-et a VALÃ“DI tool vÃ©grehajtÃ¡shoz.
        PLUS: Intelligent workflow integrÃ¡ciÃ³ a komplex feladatokhoz.
        """
        start_time = time.time()
        
        # ðŸš€ ÃšJ: Intelligent workflow ellenÅ‘rzÃ©s
        if INTELLIGENT_WORKFLOWS_AVAILABLE:
            try:
                workflow_result = await process_with_intelligent_workflow(query)
                if workflow_result.get("workflow_detected"):
                    # Ha intelligent workflow-t detektÃ¡ltunk, hasznÃ¡ljuk azt
                    logger.info(f"ðŸŽ¯ Intelligent workflow hasznÃ¡lata: {query}")
                    execution_time = time.time() - start_time
                    
                    # Intelligent workflow eredmÃ©ny formÃ¡zÃ¡sa
                    if workflow_result.get("success"):
                        return {
                            "status": "success",
                            "command_type": "INTELLIGENT_WORKFLOW", 
                            "execution_result": workflow_result,
                            "ai_summary": f"âœ… Intelligent workflow sikeresen vÃ©grehajtva: {workflow_result.get('workflow_type', 'unknown')}",
                            "execution_type": "INTELLIGENT_WORKFLOW",
                            "response_time": execution_time,
                            "model_used": model or "intelligent_workflow_system"
                        }
                    else:
                        # Ha a workflow hibÃ¡zott, folytassuk a hagyomÃ¡nyos feldolgozÃ¡ssal
                        logger.warning(f"âš ï¸ Intelligent workflow failed: {workflow_result.get('error', 'Unknown error')}")
                        
            except Exception as e:
                logger.warning(f"âš ï¸ Intelligent workflow processing failed: {e}")
                # Continue with traditional processing
        
        try:
            # 1. FÃZIS: AI elemzÃ©s Ã©s parancs felismerÃ©s
            analysis_prompt = f"""
            Elemezd ezt a feladatot Ã©s hatÃ¡rozd meg, milyen konkrÃ©t parancsokat kell vÃ©grehajtani:
            
            FELADAT: {query}
            
            ElÃ©rhetÅ‘ parancs tÃ­pusok:
            - ASK: AI kÃ©rdÃ©s (pl. 'explain something', 'analyze this')
            - CMD: Shell parancs (pl. 'list files', 'run command')
            - FILE: FÃ¡jl mÅ±veletek (pl. 'read file', 'write file', 'create file')
            - CODE: KÃ³d mÅ±veletek (pl. 'generate code', 'execute python')
            
            VÃ¡laszolj ebben a formÃ¡tumban:
            COMMAND_TYPE: [ASK/CMD/FILE/CODE]
            COMMAND_ACTION: [specific action]
            PARAMETERS: [specific parameters needed]
            """
            if model is None:
                model = await self.select_model_for_task(query, task_type or "planning")
            analysis_result = await self.ai_client.generate_response(
                prompt=analysis_prompt,
                system_message="Te egy parancs elemzÅ‘ vagy. LegyÃ©l specifikus Ã©s pontos.",
                model=model,
                task_type="planning"
            )
            # Handle both string and dict responses
            if isinstance(analysis_result, dict):
                ai_response = analysis_result.get("content", "")
            else:
                ai_response = str(analysis_result)
            logger.info(f"AI parancs elemzÃ©s: {ai_response[:200]}...")
            # 2. FÃZIS: Parancs tÃ­pus Ã©s paramÃ©terek kinyerÃ©se            command_type = "ASK"  # alapÃ©rtelmezett
            command_action = query
            parameters = {}
            lines = ai_response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('COMMAND_TYPE:'):
                    command_type = line.replace('COMMAND_TYPE:', '').strip()
                elif line.startswith('COMMAND_ACTION:'):
                    command_action = line.replace('COMMAND_ACTION:', '').strip()
                elif line.startswith('PARAMETERS:'):
                    params_text = line.replace('PARAMETERS:', '').strip()
                    # --- ÃšJ: JSON Ã©s egyszerÅ± paramÃ©ter felismerÃ©se ---
                    try:
                        # PrÃ³bÃ¡ljuk JSON-kÃ©nt Ã©rtelmezni
                        if params_text.startswith('{') and params_text.endswith('}'):
                            import json
                            parameters = json.loads(params_text)
                        else:
                            # EgyszerÅ± string feldolgozÃ¡s
                            # SpeciÃ¡lis kezelÃ©s a "filename = xyz.txt" formÃ¡tumra
                            if 'filename' in params_text.lower() and '=' in params_text:
                                # Extract filename after "filename = "
                                filename_part = params_text.split('=', 1)[1].strip()
                                if filename_part.endswith(('.txt', '.md', '.log', '.py', '.json', '.html', '.css', '.js')):
                                    parameters['path'] = filename_part
                            elif 'content' in params_text.lower() and '=' in params_text:
                                # Extract content after "content = "
                                content_part = params_text.split('=', 1)[1].strip()
                                parameters['content'] = content_part
                            else:
                                # Ha tÃ¶bb fÃ¡jlnÃ©v Ã©s/vagy tartalom is van vesszÅ‘vel elvÃ¡lasztva
                                parts = [p.strip() for p in params_text.split(',') if p.strip()]
                                file_names = [p for p in parts if p.endswith('.txt') or p.endswith('.md') or p.endswith('.log')]
                                other_parts = [p for p in parts if not (p.endswith('.txt') or p.endswith('.md') or p.endswith('.log'))]
                                if len(file_names) == 1:
                                    parameters['path'] = file_names[0]
                                elif len(file_names) >= 2:
                                    parameters['path'] = file_names[0]
                                    parameters['target_path'] = file_names[1]
                                # Ha van explicit content
                                for p in other_parts:
                                    if p.lower().startswith('content='):
                                        parameters['content'] = p[len('content='):].strip()
                                    elif p:
                                        parameters['content'] = p  # fallback: bÃ¡rmilyen szÃ¶veg
                                if not parameters and params_text:
                                    parameters['data'] = params_text
                    except json.JSONDecodeError:
                        # Ha a JSON parsing nem sikerÃ¼l, visszatÃ©rÃ¼nk az egyszerÅ± feldolgozÃ¡sra
                        parts = [p.strip() for p in params_text.split(',') if p.strip()]
                        if params_text:
                            parameters['data'] = params_text
            logger.info(f"Ã‰szlelt parancs: {command_type}, akciÃ³: {command_action}, paramÃ©terek: {parameters}")
            # 3. FÃZIS: VÃ©grehajtÃ¡s a core_old rendszerrel
            execution_result = None
            if command_type.upper() == "CMD":
                cmd = command_action if command_action else "echo Hello from Project-S"
                execution_result = await core_execution_bridge.execute_shell_command(cmd)
            elif command_type.upper() == "FILE":
                # FÃ¡jl mÅ±velet vÃ©grehajtÃ¡s
                # ParamÃ©terek normalizÃ¡lÃ¡sa                # --- JAVÃTOTT LOGIKA ---
                action = "write"
                path = None
                content = None
                # Ha a PARAMETERS csak egy fÃ¡jlnevet tartalmaz
                if 'path' in parameters:
                    path = parameters['path']
                elif 'data' in parameters and isinstance(parameters['data'], str) and parameters['data'].endswith('.txt'):
                    path = parameters['data']
                else:
                    # Intelligens fÃ¡jlnÃ©v kinyerÃ©s a query-bÃ³l
                    path = self._extract_filename_from_query(query)
                content = parameters.get('content', f"Project-S output: {query}")
                # --- ACTION LOGIKA ---
                if "create" in command_action.lower():
                    action = "write"  # 'create file' helyett 'write' kell ide!
                elif "read" in command_action.lower() or "olvas" in query.lower():
                    action = "read"
                    content = None
                # Mindig legyen action Ã©s path
                file_command = {"action": action, "path": path}
                if content is not None:
                    file_command["content"] = content
                execution_result = await core_execution_bridge.execute_file_operation(file_command)
                if isinstance(execution_result, str):
                    execution_result = {"status": "success", "content": execution_result, "path": path}
            elif command_type.upper() == "CODE":
                action = "generate"
                if "execute" in command_action.lower() or "run" in command_action.lower():
                    action = "execute"
                execution_result = await core_execution_bridge.execute_code_operation(
                    action=action,
                    description=command_action
                )
            else:
                ai_query = command_action if command_action else query
                execution_result = await core_execution_bridge.ask_ai(ai_query)
            execution_time = time.time() - start_time
            if execution_result and isinstance(execution_result, dict) and execution_result.get("status") == "success":
                # --- AI magyarÃ¡zÃ³ szÃ¶veg hozzÃ¡adÃ¡sa az eredmÃ©nyhez ---
                ai_summary = None
                try:
                    ai_summary = await self.ai_client.generate_response(
                        prompt=f"A kÃ¶vetkezÅ‘ feladatot sikeresen vÃ©grehajtottad: {query}\n\nÃrd le rÃ¶viden, hogy mit csinÃ¡lt a rendszer, Ã©s mi lett az eredmÃ©ny!",
                        system_message="Feladat vÃ©grehajtÃ¡si Ã¶sszefoglalÃ³. LÃ©gy rÃ¶vid, informatÃ­v, magyarul vÃ¡laszolj!",
                        model=model,
                        task_type="summary"
                    )
                except Exception as e:
                    ai_summary = f"(Nem sikerÃ¼lt AI Ã¶sszefoglalÃ³t generÃ¡lni: {e})"
                return {
                    "status": "success",
                    "command_type": command_type,
                    "command_action": command_action,
                    "ai_analysis": ai_response[:200] + "..." if len(ai_response) > 200 else ai_response,
                    "execution_result": execution_result,
                    "ai_summary": ai_summary if isinstance(ai_summary, str) else getattr(ai_summary, 'content', str(ai_summary)),
                    "execution_type": "CORE_OLD_SYSTEM",
                    "response_time": execution_time,
                    "model_used": model
                }
            else:
                fallback_result = await self.execute_task_with_model(
                    query=query,
                    system_message=system_message,
                    model=model,
                    task_type=task_type,
                    temperature=temperature
                )
                # Robust error extraction
                if isinstance(execution_result, dict):
                    exec_error = execution_result.get("error")
                else:
                    exec_error = str(execution_result)
                return {
                    "status": "fallback",
                    "command_type": command_type,
                    "ai_analysis": ai_response[:200] + "..." if len(ai_response) > 200 else ai_response,
                    "execution_error": exec_error,
                    "fallback_response": fallback_result,
                    "execution_type": "AI_FALLBACK",
                    "response_time": execution_time,
                    "model_used": model
                }
        except Exception as e:
            logger.error(f"Error in core system execution: {e}")
            execution_time = time.time() - start_time
            try:
                fallback_result = await self.execute_task_with_model(
                    query=query,
                    system_message=system_message,
                    model=model,
                    task_type=task_type,
                    temperature=temperature
                )
                return {
                    "status": "error_fallback",
                    "error": str(e),
                    "fallback_response": fallback_result,
                    "execution_type": "ERROR_FALLBACK",
                    "response_time": execution_time,
                    "model_used": model
                }
            except Exception as fallback_error:
                return {
                    "status": "complete_failure",
                    "error": str(e),
                    "fallback_error": str(fallback_error),
                    "execution_type": "COMPLETE_FAILURE",
                    "response_time": execution_time
                }
    
    def _get_default_model_from_config(self) -> str:
        """
        BetÃ¶lti az alapÃ©rtelmezett modellt a konfigurÃ¡ciÃ³bÃ³l.
        
        Returns:
            str: Az alapÃ©rtelmezett modell azonosÃ­tÃ³ja
        """
        try:
            # Try to get default model from ai_client config
            if hasattr(self.ai_client, 'config') and self.ai_client.config:
                default_model = self.ai_client.config.get("default_model", "qwen3-235b")
                logger.info(f"AlapÃ©rtelmezett modell konfigurÃ¡ciÃ³bÃ³l betÃ¶ltve: {default_model}")
                return default_model
        except Exception as e:
            logger.warning(f"Nem sikerÃ¼lt betÃ¶lteni az alapÃ©rtelmezett modellt a konfigurÃ¡ciÃ³bÃ³l: {e}")
          # Fallback to qwen3-235b (intended primary model)
        logger.info("AlapÃ©rtelmezett modell: qwen3-235b (fallback)")
        return "qwen3-235b"
    
    def _extract_filename_from_query(self, query: str) -> str:
        """
        Intelligens fÃ¡jlnÃ©v kinyerÃ©s a felhasznÃ¡lÃ³i kÃ©rdÃ©sbÅ‘l.
        
        Args:
            query (str): A felhasznÃ¡lÃ³i lekÃ©rdezÃ©s
            
        Returns:
            str: A kinyert fÃ¡jlnÃ©v vagy alapÃ©rtelmezett "project_s_output.txt"
        """
        import re
        
        # 1. KÃ¶zvetlenÃ¼l megadott fÃ¡jlnevek keresÃ©se (idÃ©zÅ‘jelek kÃ¶zÃ¶tt vagy kiterjesztÃ©ssel)
        # KeresÃ©s idÃ©zÅ‘jelek kÃ¶zÃ¶tt: "filename.txt", 'filename.py'
        quoted_files = re.findall(r'["\']([^"\']+\.[a-zA-Z0-9]+)["\']', query)
        if quoted_files:
            return quoted_files[0]
        
        # 2. KiterjesztÃ©ssel rendelkezÅ‘ szavak keresÃ©se
        # Pattern: szÃ³.kiterjesztÃ©s (pl. "test.txt", "main.py", "config.json")
        extension_files = re.findall(r'\b([a-zA-Z0-9_-]+\.[a-zA-Z0-9]+)\b', query)
        if extension_files:
            # KizÃ¡rjuk a domain neveket Ã©s URL-eket
            filtered_files = [f for f in extension_files if not f.startswith(('www.', 'http', 'https'))]
            if filtered_files:
                return filtered_files[0]
        
        # 3. FÃ¡jlnÃ©v kulcsszavak alapjÃ¡n
        filename_patterns = {
            # Magyar kulcsszavak
            r'\b(?:hozz\s*lÃ©tre|kÃ©szÃ­ts|Ã­rj)\s+(?:egy\s+)?([a-zA-Z0-9_-]+)\s+(?:fÃ¡jlt|file)': r'\1.txt',
            r'\b(?:nevÅ±|nÃ©ven)\s+([a-zA-Z0-9_-]+)(?:\s+fÃ¡jl)?': r'\1.txt',
            r'\b([a-zA-Z0-9_-]+)\s+(?:nevÅ±|nÃ©ven)\s+fÃ¡jl': r'\1.txt',
            # Angol kulcsszavak
            r'\b(?:create|make|write)\s+(?:a\s+)?(?:file\s+)?(?:called\s+|named\s+)?([a-zA-Z0-9_-]+)': r'\1.txt',
            r'\b(?:file\s+)?(?:called\s+|named\s+)([a-zA-Z0-9_-]+)': r'\1.txt',
            r'\b([a-zA-Z0-9_-]+)\s+(?:file|fÃ¡jl)': r'\1.txt',
        }
        
        for pattern, replacement in filename_patterns.items():
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                filename = re.sub(pattern, replacement, matches[0], flags=re.IGNORECASE)
                # TisztÃ­tjuk a fÃ¡jlnevet
                clean_filename = re.sub(r'[^\w\.-]', '_', filename)
                if clean_filename and not clean_filename.startswith('.'):
                    return clean_filename
        
        # 4. SpeciÃ¡lis esetek - gyakori fÃ¡jltÃ­pusok felismerÃ©se
        content_type_mapping = {
            r'\b(?:html|webpage|weboldal)': 'index.html',
            r'\b(?:css|stylesheet|stÃ­lus)': 'style.css',
            r'\b(?:js|javascript)': 'script.js',
            r'\b(?:py|python)': 'main.py',
            r'\b(?:json|config|konfig)': 'config.json',
            r'\b(?:md|markdown|readme)': 'README.md',
            r'\b(?:txt|text|szÃ¶veg)': 'document.txt',
            r'\b(?:log|naplÃ³)': 'log.txt',
            r'\b(?:csv|tÃ¡blÃ¡zat)': 'data.csv',
        }
        
        for pattern, filename in content_type_mapping.items():
            if re.search(pattern, query, re.IGNORECASE):
                return filename
        
        # 5. UtolsÃ³ esÃ©ly: kulcsszavak alapjÃ¡n generÃ¡lÃ¡s
        if any(word in query.lower() for word in ['list', 'lista', 'jegyzÃ©k']):
            return 'lista.txt'
        elif any(word in query.lower() for word in ['note', 'jegyzet', 'memo']):
            return 'jegyzet.txt'
        elif any(word in query.lower() for word in ['test', 'teszt', 'prÃ³ba']):
            return 'test.txt'
        elif any(word in query.lower() for word in ['hello', 'hellÃ³', 'Ã¼dvÃ¶zlet']):
            return 'hello.txt'
          # 6. AlapÃ©rtelmezett visszatÃ©rÃ©s
        logger.info(f"Nem sikerÃ¼lt kinyerni a fÃ¡jlnevet a query-bÃ³l: '{query}', alapÃ©rtelmezett hasznÃ¡lata")
        return "project_s_output.txt"
    
    async def process_user_command(self, user_input: str) -> dict:
        """
        Process user command - main entry point for CLI commands.
        Routes to the appropriate execution method based on command type.
        """
        try:
            logger.info(f"Processing user command: {user_input}")
            
            # Use the core system for execution
            result = await self.execute_task_with_core_system(
                query=user_input,
                model="qwen3-235b"  # Default model
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing user command: {e}")
            return {
                "status": "error",
                "error": str(e),
                "command": user_input
            }
    
# Singleton pÃ©ldÃ¡ny lÃ©trehozÃ¡sa
model_manager = ModelManager()
