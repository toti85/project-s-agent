"""
Project-S Modell Menedzser
-------------------------
Ez a modul kezeli az AI modell kiv√°laszt√°s√°t, v√°lt√°st √©s a modellek teljes√≠tm√©ny√©nek nyomon k√∂vet√©s√©t.
"""

import os
import logging
import asyncio
import json
from typing import Dict, Any, List, Optional, Union, Tuple
import time
from datetime import datetime
from pathlib import Path

from integrations.multi_model_ai_client import multi_model_ai_client
from integrations.persistent_state_manager import persistent_state_manager
from integrations.core_execution_bridge import core_execution_bridge

# Tool integration for actual task execution
logger = logging.getLogger(__name__)

try:
    from tools.tool_registry import tool_registry
    TOOLS_AVAILABLE = True
    logger.info("Tool registry imported successfully - tools available for execution")
except ImportError as e:
    TOOLS_AVAILABLE = False
    logger.warning(f"Tool registry not available: {e}")

# Intelligent workflow integration
try:
    from integrations.intelligent_workflow_integration import (
        intelligent_workflow_orchestrator,
        process_with_intelligent_workflow
    )
    INTELLIGENT_WORKFLOWS_AVAILABLE = True
    logger.info("‚úÖ Intelligent workflow integration imported successfully")
except ImportError as e:
    INTELLIGENT_WORKFLOWS_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Intelligent workflow integration not available: {e}")

class ModelManager:
    """
    Modell menedzser oszt√°ly a modellek kezel√©s√©hez √©s intelligens modellv√°laszt√°shoz.
    """
    
    def __init__(self):
        """Modell menedzser inicializ√°l√°sa."""
        self.ai_client = multi_model_ai_client
        
        # Modell metrik√°k
        self.model_metrics = {}
          # Alap√©rtelmezett modell (config-b√≥l bet√∂ltve)
        self.default_model = self._get_default_model_from_config()
        
        # Modell haszn√°lat k√∂vet√©se
        self.model_usage = {}
        
        # Utols√≥ modellv√°lt√°s ideje
        self.last_model_switch_time = None
        
        # Modell teljes√≠tm√©ny cache
        self.performance_cache_path = Path(__file__).parent.parent / "memory" / "model_performance_cache.json"
        self.performance_cache = self._load_performance_cache()
        
        # Hozz√°f√©r√©s a perzisztens √°llapotkezel≈ëh√∂z
        self.state_manager = persistent_state_manager
        
        logger.info("Modell menedzser inicializ√°lva")
        
    def _load_performance_cache(self) -> Dict[str, Any]:
        """Bet√∂lti a modell teljes√≠tm√©ny cache-t, ha l√©tezik."""
        try:
            if self.performance_cache_path.exists():
                with open(self.performance_cache_path, 'r', encoding='utf-8') as file:
                    return json.load(file)
            return {}
        except Exception as e:
            logger.error(f"Hiba a teljes√≠tm√©ny cache bet√∂lt√©se k√∂zben: {e}")
            return {}
    
    def _save_performance_cache(self) -> None:
        """Elmenti a modell teljes√≠tm√©ny cache-t."""
        try:
            # Biztos√≠tjuk, hogy a k√∂nyvt√°r l√©tezik
            self.performance_cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.performance_cache_path, 'w', encoding='utf-8') as file:
                json.dump(self.performance_cache, file, indent=2)
                
        except Exception as e:
            logger.error(f"Hiba a teljes√≠tm√©ny cache ment√©se k√∂zben: {e}")
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """
        Visszaadja az el√©rhet≈ë modellek list√°j√°t.
        
        Returns:
            List[Dict]: Az el√©rhet≈ë modellek list√°ja
        """
        return self.ai_client.list_available_models()
        
    def determine_task_type(self, query: str) -> str:
        """
        Meghat√°rozza a bemeneti lek√©rdez√©s t√≠pus√°t.
        
        Args:
            query: A felhaszn√°l√≥i lek√©rdez√©s
            
        Returns:
            str: A feladat t√≠pusa (pl. "tervez√©s", "k√≥dol√°s", stb.)
        """
        # Itt k√©s≈ëbb komplett feladat t√≠pus detekt√°l√≥ logik√°t lehet implement√°lni
          # Egyszer≈± kulcssz√≥ alap√∫ detekt√°l√°s (magyar √©s angol t√°mogat√°ssal)
        keywords = {
            "tervez√©s": ["tervez√©s", "tervezz√ºnk", "tervezd meg", "√©p√≠tsd fel", "architekt√∫ra", "planning", "plan", "design", "architecture"],
            "k√≥dol√°s": ["k√≥d", "programoz√°s", "implement√°ci√≥", "fejleszt√©s", "scriptet", "code", "coding", "programming", "development", "script"],
            "dokument√°ci√≥": ["dokument√°ci√≥", "magyar√°zd el", "dokumentumd", "√≠rj le√≠r√°st", "documentation", "document", "explain", "describe"],
            "adatelemz√©s": ["elemezd", "adatelemz√©s", "adatfeldolgoz√°s", "statisztika", "analysis", "analyze", "data analysis", "statistics"],
            "kreat√≠v_√≠r√°s": ["kreat√≠v", "t√∂rt√©net", "√≠rj egy", "fogalmaz√°s", "creative", "story", "writing", "compose"],
            "ford√≠t√°s": ["ford√≠tsd", "ford√≠t√°s", "angolul", "magyarul", "translate", "translation"],
            "√∂sszefoglal√°s": ["√∂sszefoglal√°s", "√∂sszegz√©s", "r√∂viden foglald", "t√∂m√∂r√≠ts", "summary", "summarize", "brief"],
            "gyors_v√°lasz": ["gyors", "r√∂vid", "gyorsan", "quick", "fast", "short", "briefly"]
        }
        
        # Legjobb egyez√©s keres√©se
        task_matches = {}
        query_lower = query.lower()
        
        for task_type, task_keywords in keywords.items():
            matches = sum(1 for keyword in task_keywords if keyword.lower() in query_lower)
            if matches > 0:
                task_matches[task_type] = matches
                
        if task_matches:
            # Legt√∂bb egyez√©s kiv√°laszt√°sa
            best_task = max(task_matches.items(), key=lambda x: x[1])[0]
            logger.info(f"Feladat t√≠pus meghat√°rozva: {best_task}")
            return best_task
            
        # Alap√©rtelmezettk√©nt felt√©telezz√ºk, hogy gyors v√°lasz
        logger.info("Nem ismert fel specifikus feladat t√≠pust, 'gyors_v√°lasz' haszn√°lata")
        return "gyors_v√°lasz"
    
    async def select_model_for_task(self, query: str, task_type: Optional[str] = None) -> str:
        """
        Kiv√°laszt egy megfelel≈ë modellt a feladathoz.
        
        Args:
            query: A felhaszn√°l√≥i lek√©rdez√©s
            task_type: Opcion√°lis explicit feladat t√≠pus
            
        Returns:
            str: A kiv√°lasztott modell azonos√≠t√≥ja
        """
        # Ha nincs explicit feladat t√≠pus, detekt√°ljuk
        if task_type is None:
            task_type = self.determine_task_type(query)
        # Fallback: ha nincs ilyen met√≥dus, alap√©rtelmezett modell
        if hasattr(self.ai_client, 'suggest_model_for_task'):
            suggested_model = self.ai_client.suggest_model_for_task(task_type)
        else:
            suggested_model = self.default_model
        logger.info(f"A(z) '{task_type}' feladat t√≠pushoz a(z) '{suggested_model}' modellt v√°lasztottuk")
        return suggested_model
    
    def record_model_performance(self, 
                              model_id: str, 
                              task_type: str, 
                              response_time: float, 
                              success: bool = True) -> None:
        """
        R√∂gz√≠ti egy modell teljes√≠tm√©ny√©t.
        
        Args:
            model_id: A modell azonos√≠t√≥ja
            task_type: A feladat t√≠pusa
            response_time: A v√°laszid≈ë m√°sodpercben
            success: Sikeres volt-e a k√©r√©s
        """
        # Ha a modell m√©g nem szerepel a teljes√≠tm√©ny adatb√°zisban, adjuk hozz√°
        if model_id not in self.performance_cache:
            self.performance_cache[model_id] = {}
            
        # Ha az adott feladatt√≠pushoz m√©g nincs adat, inicializ√°ljuk
        if task_type not in self.performance_cache[model_id]:
            self.performance_cache[model_id][task_type] = {
                "request_count": 0,
                "success_count": 0,
                "failure_count": 0,
                "total_response_time": 0,
                "avg_response_time": 0,
                "last_used": datetime.now().isoformat()
            }
            
        # Friss√≠tj√ºk a statisztik√°kat
        stats = self.performance_cache[model_id][task_type]
        stats["request_count"] += 1
        if success:
            stats["success_count"] += 1
        else:
            stats["failure_count"] += 1
            
        stats["total_response_time"] += response_time
        stats["avg_response_time"] = stats["total_response_time"] / stats["request_count"]
        stats["last_used"] = datetime.now().isoformat()
        
        # Elmentj√ºk a friss√≠tett cache-t
        self._save_performance_cache()
            
    async def execute_task_with_model(self, 
                                    query: str,
                                    system_message: Optional[str] = None,
                                    model: Optional[str] = None,
                                    task_type: Optional[str] = None,
                                    temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        V√©grehajt egy feladatot a legmegfelel≈ëbb modellel vagy a megadott modellel.
        
        Args:
            query: A felhaszn√°l√≥i lek√©rdez√©s
            system_message: Opcion√°lis rendszer√ºzenet
            model: Explicit modell azonos√≠t√≥, ha meg van adva
            task_type: Explicit feladat t√≠pus, ha meg van adva 
            temperature: Opcion√°lis h≈ëm√©rs√©klet be√°ll√≠t√°s
            
        Returns:
            Dict: A v√°lasz sz√≥t√°r a tartalommal √©s metaadatokkal
        """
        # Ha nincs explicit feladat t√≠pus, meghat√°rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs explicit modell, kiv√°lasztjuk
        if model is None:
            model = await self.select_model_for_task(query, task_type)
              # M√©rj√ºk a v√°laszid≈ët
        start_time = time.time()
        success = True
        
        try:
            # V√©grehajtjuk a feladatot
            result = await self.ai_client.generate_response(
                prompt=query,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            )
            
        except Exception as e:
            logger.error(f"Hiba a modell v√©grehajt√°sa k√∂zben ({model}): {e}")
            success = False
            
            # Visszaes√©s az alap√©rtelmezett modellre
            logger.info(f"Visszaes√©s az alap√©rtelmezett modellre ({self.default_model})")
            try:
                result = await self.ai_client.generate_response(
                    prompt=query,
                    system_message=system_message,
                    model=self.default_model,
                    task_type=task_type,
                    temperature=temperature
                )
                model = self.default_model
            except Exception as fallback_e:
                logger.error(f"Hiba a visszaes√©si modell v√©grehajt√°sa k√∂zben: {fallback_e}")
                result = None  # Explicit None meghat√°roz√°s
                model = self.default_model
        
        # Kisz√°moljuk a v√°laszid≈ët
        end_time = time.time()
        response_time = end_time - start_time
        
        # R√∂gz√≠tj√ºk a teljes√≠tm√©nyt
        self.record_model_performance(model, task_type, response_time, success)
        
        # Ha a result None, string vagy dict, mindent kezel√ºnk
        if result is None:
            result = {
                "response": "No response received",
                "model": model,
                "task_type": task_type,
                "success": False,
                "response_time": response_time,
                "error": "Response was None"
            }
        elif isinstance(result, str):
            result = {
                "response": result,
                "model": model,
                "task_type": task_type,
                "success": success,
                "response_time": response_time
            }
        elif isinstance(result, dict):
            # Hozz√°adjuk a v√°laszid≈ët a v√°laszhoz
            result["response_time"] = response_time
            result["model"] = model
            result["task_type"] = task_type
        else:
            # Egy√©b t√≠pusok eset√©n string reprezent√°ci√≥t haszn√°lunk
            result = {
                "response": str(result),
                "model": model,
                "task_type": task_type,
                "success": success,
                "response_time": response_time
            }
        
        return result
    
    async def run_task_with_multiple_models(self, 
                                         query: str,
                                         system_message: Optional[str] = None,
                                         models: List[str] = [],
                                         task_type: Optional[str] = None,
                                         temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        V√©grehajt egy feladatot t√∂bb k√ºl√∂nb√∂z≈ë modellel.
        
        Args:
            query: A felhaszn√°l√≥i lek√©rdez√©s
            system_message: Opcion√°lis rendszer√ºzenet
            models: A haszn√°land√≥ modellek list√°ja
            task_type: Explicit feladat t√≠pus, ha meg van adva
            temperature: Opcion√°lis h≈ëm√©rs√©klet be√°ll√≠t√°s
            
        Returns:
            Dict: A v√°laszok sz√≥t√°ra modell azonos√≠t√≥kkal
        """
        # Ha nincs explicit feladat t√≠pus, meghat√°rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs megadva modellek, akkor javaslatot k√©r√ºnk
        if not models:
            # Ha a feladathoz tartoznak aj√°nlott modellek a konfigban, haszn√°ljuk azokat
            task_models = self.ai_client.config.get("task_model_mapping", {}).get(task_type, [])
            if task_models:
                models = task_models[:2]  # Els≈ë 2 aj√°nlott modell haszn√°lata 
            else:
                # K√ºl√∂nben haszn√°ljuk az alap√©rtelmezettet √©s egy m√°sikat
                models = [self.default_model, "claude-3-sonnet"]
                
        # P√°rhuzamosan futtatjuk a lek√©rdez√©seket minden modellel
        tasks = []
        for model in models:
            tasks.append(self.execute_task_with_model(
                query=query,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            ))
            
        # V√°rjuk meg az √∂sszes feladat befejez√©s√©t
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Feldolgozzuk az eredm√©nyeket
        model_responses = {}
        for model, result in zip(models, results):
            if isinstance(result, Exception):
                model_responses[model] = {
                    "error": True,
                    "message": str(result),
                    "model": model
                }
            else:
                model_responses[model] = result
                
        return {
            "task_type": task_type,
            "models_used": models,
            "responses": model_responses
        }
    
    async def execute_task_with_tools(self, 
                                     query: str,
                                     system_message: Optional[str] = None,
                                     model: Optional[str] = None,
                                     task_type: Optional[str] = None,
                                     temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        üî• KRITIKUS JAV√çT√ÅS: V√©grehajtja a feladatot AI tervez√©ssel √âS val√≥di tool v√©grehajt√°ssal.
        
        Ez a met√≥dus hidalja √°t az AI v√°laszok √©s a val√≥di tool v√©grehajt√°sok k√∂z√∂tti h√©zagot.
        
        Args:
            query: A felhaszn√°l√≥i lek√©rdez√©s
            system_message: Opcion√°lis rendszer√ºzenet
            model: Explicit modell azonos√≠t√≥
            task_type: Explicit feladat t√≠pus
            temperature: Opcion√°lis h≈ëm√©rs√©klet be√°ll√≠t√°s
            
        Returns:
            Dict: AI v√°lasz + val√≥di tool v√©grehajt√°sok eredm√©nye
        """
        if not TOOLS_AVAILABLE:
            logger.warning("Tools not available, falling back to AI-only response")
            return await self.execute_task_with_model(query, system_message, model, task_type, temperature)
        
        # 1. F√ÅZIS: AI Elemz√©s √©s Tool Tervez√©s
        planning_prompt = f"""
        Elemezd ezt a feladatot √©s hat√°rozd meg, milyen konkr√©t tools-okat kell haszn√°lni:
        
        FELADAT: {query}
        
        El√©rhet≈ë tools: {list(tool_registry.tool_classes.keys()) if tool_registry else []}
        
        V√°laszolj ebben a form√°tumban:
        ELEMZ√âS: [r√∂vid elemz√©s]
        TOOLS_NEEDED: [tool1, tool2, tool3]
        PARAMETERS: 
        - tool1: {{param1: value1, param2: value2}}
        - tool2: {{param1: value1}}
        EXECUTION_ORDER: [tool1, tool2, tool3]
        
        Csak azokat a tools-okat haszn√°ld, amik a list√°ban vannak!
        """
        
        planning_result = await self.execute_task_with_model(
            query=planning_prompt,
            system_message="Te egy task elemz≈ë √©s tool planner vagy. Legy√©l specifikus a tool param√©terekkel.",
            model=model,
            task_type="planning"
        )
        
        # 2. F√ÅZIS: Tool Param√©terek Kinyer√©se
        ai_response = planning_result.get("content", "")
        tools_executed = []
        
        try:
            # Bet√∂ltj√ºk a tools-okat
            await tool_registry.load_tools()
            
            # Egyszer≈± parsing a tool nevekhez (ezt k√©s≈ëbb lehet jav√≠tani)
            lines = ai_response.split('\n')
            tools_to_execute = []
            
            for line in lines:
                if line.startswith('TOOLS_NEEDED:'):
                    tools_part = line.replace('TOOLS_NEEDED:', '').strip()
                    # Kivessz√ºk a tool neveket
                    tools_part = tools_part.replace('[', '').replace(']', '')
                    tool_names = [t.strip() for t in tools_part.split(',') if t.strip()]
                    tools_to_execute = tool_names
                    break
            
            logger.info(f"Tools to execute: {tools_to_execute}")
            
            # 3. F√ÅZIS: Val√≥di Tool V√©grehajt√°s
            for tool_name in tools_to_execute:
                tool_name_clean = tool_name.strip()
                
                if tool_name_clean in tool_registry.tool_classes:
                    try:
                        tool_instance = tool_registry.get_tool(tool_name_clean)
                          # Egyszer≈± param√©ter gener√°l√°s a feladat alapj√°n
                        if "FileWriteTool" in tool_name_clean and ("f√°jl" in query.lower() or "file" in query.lower() or "√≠rj" in query.lower()):
                            # F√°jl √≠r√°s - intelligens f√°jln√©v kinyer√©s
                            filename = self._extract_filename_from_query(query)
                            content = f"Project-S v√©grehajt√°s eredm√©nye:\n{query}\n\nL√©trehozva: {datetime.now()}"
                            
                            result = await tool_instance.execute(path=filename, content=content)
                            tools_executed.append({
                                "tool": tool_name_clean,
                                "parameters": {"path": filename, "content": content[:50] + "..."},
                                "result": result,
                                "success": result.get("success", False)
                            })
                            
                        elif "FileReadTool" in tool_name_clean:
                            # Pr√≥b√°ljunk olvasni egy l√©tez≈ë f√°jlt
                            import os
                            for filename in ["project_s_output.txt", "README.md", "requirements.txt"]:
                                if os.path.exists(filename):
                                    result = await tool_instance.execute(path=filename)
                                    tools_executed.append({
                                        "tool": tool_name_clean,
                                        "parameters": {"path": filename},
                                        "result": result,
                                        "success": result.get("success", False)
                                    })
                                    break
                                    
                        elif "SystemCommandTool" in tool_name_clean:
                            # Biztons√°gos parancs v√©grehajt√°s
                            safe_commands = {
                                "lista": "dir",
                                "list": "dir", 
                                "files": "dir",
                                "hello": "echo Hello from Project-S",
                                "test": "echo Tool execution working!"
                            }
                            
                            command = "echo Tool execution successful!"
                            for keyword, cmd in safe_commands.items():
                                if keyword in query.lower():
                                    command = cmd
                                    break
                            
                            result = await tool_instance.execute(command=command)
                            tools_executed.append({
                                "tool": tool_name_clean,
                                "parameters": {"command": command},
                                "result": result,
                                "success": result.get("success", False)
                            })
                            
                        else:
                            logger.info(f"Tool {tool_name_clean} requires specific parameter handling")
                            
                    except Exception as e:
                        logger.error(f"Error executing tool {tool_name_clean}: {e}")
                        tools_executed.append({
                            "tool": tool_name_clean,
                            "error": str(e),
                            "success": False
                        })
                else:
                    logger.warning(f"Tool {tool_name_clean} not found in registry")
            
        except Exception as e:
            logger.error(f"Error in tool execution phase: {e}")
        
        # 4. F√ÅZIS: Eredm√©ny √ñsszegz√©s
        summary_prompt = f"""
        Eredeti feladat: {query}
        
        AI tervez√©s: {ai_response[:200]}...
        
        V√©grehajtott tools: {len(tools_executed)}
        Tool eredm√©nyek: {[t.get('success', False) for t in tools_executed]}
        
        K√©sz√≠ts egy r√∂vid √∂sszegz√©st: mi t√∂rt√©nt val√≥j√°ban, milyen f√°jlok/m≈±veletek j√∂ttek l√©tre.
        """
        
        summary_result = await self.execute_task_with_model(
            query=summary_prompt,
            system_message="K√©sz√≠ts r√∂vid, faktikus √∂sszegz√©st a v√©grehajtott m≈±veletekr≈ël.",
            model=model,
            task_type="summary"
        )
        
        # Teljes eredm√©ny visszaad√°sa
        return {
            "ai_planning": planning_result,
            "tools_executed": tools_executed,
            "tools_count": len(tools_executed),
            "tools_successful": len([t for t in tools_executed if t.get('success', False)]),
            "summary": summary_result,
            "execution_type": "AI_PLUS_TOOLS",  # Jelzi, hogy ez val√≥di v√©grehajt√°s volt
            "response_time": planning_result.get("response_time", 0)
        }
    
    async def get_model_performance_stats(self, model_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Visszaadja a modell(ek) teljes√≠tm√©ny statisztik√°it.
        
        Args:
            model_id: Opcion√°lis modell azonos√≠t√≥, ha csak egy modellre vagyunk k√≠v√°ncsiak
            
        Returns:
            Dict: A modellek teljes√≠tm√©ny statisztik√°i
        """
        if model_id:
            # Ha csak egy modellet k√©r√ºnk
            if model_id in self.performance_cache:
                return {model_id: self.performance_cache[model_id]}
            return {model_id: "Nincs teljes√≠tm√©ny adat a modellhez"}
            
        # Minden modellhez visszaadjuk a statisztik√°kat
        return self.performance_cache

    async def execute_task_with_model_in_session(self, 
                                         session_id: str,
                                         query: str,
                                         system_message: Optional[str] = None,
                                         model: Optional[str] = None,
                                         task_type: Optional[str] = None,
                                         temperature: Optional[float] = None,
                                         persist_history: bool = True) -> Dict[str, Any]:
        """
        V√©grehajtja a feladatot egy adott modellel, egy adott munkamenetben,
        opcion√°lisan mentve a besz√©lget√©s t√∂rt√©net√©t.
        
        Args:
            session_id: A munkamenet azonos√≠t√≥ja
            query: A felhaszn√°l√≥i lek√©rdez√©s
            system_message: Opcion√°lis rendszer√ºzenet
            model: Opcion√°lis modell azonos√≠t√≥ (ha nincs megadva, kiv√°laszt√°sra ker√ºl)
            task_type: Opcion√°lis feladat t√≠pus
            temperature: Opcion√°lis h≈ëm√©rs√©klet √©rt√©k
            persist_history: Ha True, a besz√©lget√©s beker√ºl a perzisztens t√°rol√≥ba
            
        Returns:
            Dict: A v√°lasz sz√≥t√°r a tartalommal √©s metaadatokkal
        """
        # Az el≈ëz≈ë besz√©lget√©sek bet√∂lt√©se a munkamenetb≈ël
        conversation_history = []
        if persist_history:
            conversation_history = await self.state_manager.get_conversation_history(session_id)
        
        # Ha nincs explicit feladat t√≠pus, meghat√°rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs explicit modell, kiv√°lasztjuk
        if model is None:
            model = await self.select_model_for_task(query, task_type)
        
        # Ha van besz√©lget√©si el≈ëzm√©ny, hozz√°adjuk a prompt elej√©hez
        enhanced_prompt = query
        if conversation_history:
            # Az utols√≥ n√©h√°ny besz√©lget√©s hozz√°ad√°sa kontextusk√©nt
            recent_history = conversation_history[-5:]  # Utols√≥ 5 besz√©lget√©s
            context = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in recent_history
            ])
            enhanced_prompt = f"El≈ëzm√©nyek:\n{context}\n\nAktu√°lis k√©rd√©s:\n{query}"
            
        # M√©rj√ºk a v√°laszid≈ët
        start_time = time.time()
        success = True
        
        try:
            # V√©grehajtjuk a feladatot
            result = await self.ai_client.generate_response(
                prompt=enhanced_prompt,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            )
            
        except Exception as e:
            logger.error(f"Hiba a modell v√©grehajt√°sa k√∂zben ({model}): {e}")
            success = False
            
            # Visszaes√©s az alap√©rtelmezett modellre
            logger.info(f"Visszaes√©s az alap√©rtelmezett modellre ({self.default_model})")
            result = await self.ai_client.generate_response(
                prompt=enhanced_prompt,
                system_message=system_message,
                model=self.default_model,
                task_type=task_type,
                temperature=temperature
            )
            model = self.default_model
        
        # Kisz√°moljuk a v√°laszid≈ët
        end_time = time.time()
        response_time = end_time - start_time
        
        # R√∂gz√≠tj√ºk a teljes√≠tm√©nyt
        self.record_model_performance(
            model_id=model,
            task_type=task_type,
            response_time=response_time,
            success=success
        )
          # A v√°laszok ment√©se a perzisztens t√°rol√≥ba, ha sz√ºks√©ges
        if persist_history:
            # Felhaszn√°l√≥i √ºzenet ment√©se
            await self.state_manager.add_conversation_entry(
                session_id=session_id, 
                role="user", 
                content=query,
                metadata={"task_type": task_type}
            )
            
            # Handle both string and dictionary responses
            if isinstance(result, str):
                content = result
                provider = self._get_provider_for_model(model)
            else:
                content = result.get("content", str(result)) if result else "No response"
                provider = result.get("provider") if result else None
            
            # Asszisztens v√°lasz√°nak ment√©se
            await self.state_manager.add_conversation_entry(
                session_id=session_id, 
                role="assistant", 
                content=content,
                metadata={
                    "model": model,
                    "provider": provider,
                    "response_time": response_time
                }
            )
        
        # Visszaadjuk az eredm√©nyt a v√°laszid≈ëvel √©s egy√©b metaadatokkal
        if isinstance(result, str):
            return {
                "content": result,
                "model": model,
                "provider": self._get_provider_for_model(model),
                "response_time": response_time,
                "session_id": session_id
            }
        elif isinstance(result, dict):
            result.update({
                "response_time": response_time,
                "session_id": session_id
            })
            return result
        else:
            return {
                "content": str(result) if result else "No response",
                "model": model,
                "provider": self._get_provider_for_model(model),
                "response_time": response_time,
                "session_id": session_id
            }
    
    async def run_task_with_multiple_models_in_session(self, 
                                                    session_id: str,
                                                    query: str,
                                                    system_message: Optional[str] = None,
                                                    models: List[str] = [],
                                                    task_type: Optional[str] = None,
                                                    temperature: Optional[float] = None,
                                                    persist_history: bool = True) -> Dict[str, Any]:
        """
        V√©grehajtja a feladatot t√∂bb modellel egy adott munkamenetben,
        √©s √∂sszehasonl√≠tja az eredm√©nyeiket.
        
        Args:
            session_id: A munkamenet azonos√≠t√≥ja
            query: A felhaszn√°l√≥i lek√©rdez√©s
            system_message: Opcion√°lis rendszer√ºzenet
            models: A haszn√°land√≥ modellek list√°ja. Ha √ºres, automatikusan kiv√°laszt n√©h√°nyat.
            task_type: Opcion√°lis feladat t√≠pus
            temperature: Opcion√°lis h≈ëm√©rs√©klet √©rt√©k
            persist_history: Ha True, a besz√©lget√©s beker√ºl a perzisztens t√°rol√≥ba
            
        Returns:
            Dict: Sz√≥t√°r a k√ºl√∂nb√∂z≈ë modellek v√°laszaival
        """
        # Az el≈ëz≈ë besz√©lget√©sek bet√∂lt√©se a munkamenetb≈ël
        conversation_history = []
        if persist_history:
            conversation_history = await self.state_manager.get_conversation_history(session_id)
            
        # Ha nincs explicit feladat t√≠pus, meghat√°rozzuk
        if task_type is None:
            task_type = self.determine_task_type(query)
            
        # Ha nincs megadva modell, akkor a feladatt√≠pusra aj√°nlott modelleket haszn√°ljuk
        if not models:
            task_models = self.ai_client.config.get("task_model_mapping", {}).get(task_type, [])
            if task_models:
                models = task_models[:2]  # Els≈ë 2 aj√°nlott modell haszn√°lata 
            else:
                # K√ºl√∂nben haszn√°ljuk az alap√©rtelmezettet √©s egy m√°sikat
                models = [self.default_model, "claude-3-sonnet"]
        
        # Ha van besz√©lget√©si el≈ëzm√©ny, hozz√°adjuk a prompt elej√©hez
        enhanced_prompt = query
        if conversation_history:
            # Az utols√≥ n√©h√°ny besz√©lget√©s hozz√°ad√°sa kontextusk√©nt
            recent_history = conversation_history[-5:]  # Utols√≥ 5 besz√©lget√©s
            context = "\n".join([
                f"{msg['role']}: {msg['content']}" 
                for msg in recent_history
            ])
            enhanced_prompt = f"El≈ëzm√©nyek:\n{context}\n\nAktu√°lis k√©rd√©s:\n{query}"
                
        # P√°rhuzamosan futtatjuk a lek√©rdez√©seket minden modellel
        tasks = []
        for model in models:
            task = self.ai_client.generate_response(
                prompt=enhanced_prompt,
                system_message=system_message,
                model=model,
                task_type=task_type,
                temperature=temperature
            )
            tasks.append(task)
            
        # V√°rjuk meg az √∂sszes eredm√©nyt
        results = await asyncio.gather(*tasks, return_exceptions=True)
          # Feldolgozzuk az eredm√©nyeket
        model_responses = {}
        for i, result in enumerate(results):
            model = models[i]
            
            # Ellen≈ërizz√ºk, hogy sikeres volt-e a v√©grehajt√°s
            if isinstance(result, Exception):
                model_responses[model] = {
                    "error": True,
                    "message": str(result),
                    "model": model
                }
            elif result is None:
                model_responses[model] = {
                    "error": True,
                    "message": "No response received",
                    "model": model
                }
            else:
                # Ha a v√°lasz string, akkor csomagoljuk be dictionary-ba
                if isinstance(result, str):
                    model_responses[model] = {
                        "content": result,
                        "model": model,
                        "success": True
                    }
                else:
                    model_responses[model] = result
                    
                    # R√∂gz√≠tj√ºk a teljes√≠tm√©nyt sikeres futtat√°sn√°l
                    if isinstance(result, dict) and "response_time" in result:
                        self.record_model_performance(
                            model_id=model,
                            task_type=task_type,
                            response_time=result["response_time"],
                            success=True
                        )
                
        # Elmentj√ºk a felhaszn√°l√≥i k√©rd√©st, ha sz√ºks√©ges
        if persist_history:
            await self.state_manager.add_conversation_entry(
                session_id=session_id, 
                role="user", 
                content=query,
                metadata={"task_type": task_type}
            )
              # Elmentj√ºk az √∂sszes asszisztens v√°laszt
            for model, response in model_responses.items():
                if not isinstance(response, dict) or "error" in response:
                    continue
                    
                await self.state_manager.add_conversation_entry(
                    session_id=session_id, 
                    role="assistant", 
                    content=response.get("content", ""),
                    metadata={
                        "model": model,
                        "provider": response.get("provider"),
                        "multi_model_comparison": True,
                        "response_time": response.get("response_time")
                    }
                )
        
        return {
            "task_type": task_type,
            "models_used": models,
            "responses": model_responses,
            "session_id": session_id
        }
    
    def _get_provider_for_model(self, model_id: str) -> Optional[str]:
        """Get the provider for a specific model."""
        return self.ai_client._get_provider_for_model(model_id)

    def _detect_workflow_task(self, query: str) -> Optional[Dict[str, Any]]:
        """
        Detect if a query requires a complex multi-step workflow.
        
        Args:
            query (str): The user query to analyze
            
        Returns:
            Optional[Dict[str, Any]]: Workflow configuration if detected, None otherwise
        """
        query_lower = query.lower().strip()
        
        # File organization workflow patterns
        file_org_patterns = [
            "organize", "organise", "rendezd", "rendszerezd",
            "sort files", "clean up", "kategoriz√°ld", "sort√≠rozd",
            "by file type", "file types", "t√≠pus szerint", "kiterjeszt√©s szerint",
            "downloads", "let√∂lt√©sek", "download folder",
            "remove duplicates", "dublicate", "duplik√°lt", "ism√©tl≈ëd≈ë",
            "clean folder", "mappa takar√≠t√°s"
        ]
        
        folder_patterns = [
            "downloads", "download", "let√∂lt√©sek", "desktop", "asztal",
            "documents", "dokumentumok", "pictures", "k√©pek", "videos", "vide√≥k"
        ]
        
        # Check for file organization patterns
        if any(pattern in query_lower for pattern in file_org_patterns):
            # Extract folder path if mentioned
            folder_path = None
            
            # Look for specific folder mentions
            for folder in folder_patterns:
                if folder in query_lower:
                    if folder in ["downloads", "download", "let√∂lt√©sek"]:
                        folder_path = os.path.expanduser("~/Downloads")
                    elif folder in ["desktop", "asztal"]:
                        folder_path = os.path.expanduser("~/Desktop")
                    elif folder in ["documents", "dokumentumok"]:
                        folder_path = os.path.expanduser("~/Documents")
                    elif folder in ["pictures", "k√©pek"]:
                        folder_path = os.path.expanduser("~/Pictures")
                    elif folder in ["videos", "vide√≥k"]:
                        folder_path = os.path.expanduser("~/Videos")
                    break
            
            # If no specific folder found, try to extract path from query
            if not folder_path:
                import re
                # Look for path patterns like C:\Users\... or /home/...
                path_match = re.search(r'["\']?([A-Za-z]:[\\\/][^"\']+|\/[^"\']+)["\']?', query)
                if path_match:
                    folder_path = path_match.group(1).strip('\'"')
                else:
                    # Default to Downloads folder
                    folder_path = os.path.expanduser("~/Downloads")
            
            # Determine organization type
            organization_type = "by_type"  # default
            if any(pattern in query_lower for pattern in ["by date", "date", "d√°tum szerint", "id≈ërend"]):
                organization_type = "by_date"
            
            # Check for duplicate removal request
            remove_duplicates = any(pattern in query_lower for pattern in [
                "remove duplicates", "delete duplicates", "duplik√°lt", "ism√©tl≈ëd≈ë", "duplicate"
            ])
            
            logger.info(f"üìÅ File organization workflow detected - Path: {folder_path}, Type: {organization_type}, Remove duplicates: {remove_duplicates}")
            
            return {
                "type": "file_organization",
                "task_data": {
                    "type": "file_organization",
                    "path": folder_path,
                    "organization_type": organization_type,
                    "remove_duplicates": remove_duplicates,
                    "description": query
                },
                "path": folder_path,
                "organization_type": organization_type,
                "remove_duplicates": remove_duplicates
            }
        
        # Code analysis workflow patterns
        code_analysis_patterns = [
            "analyze code", "code review", "elemezd a k√≥dot", "k√≥d √°ttekint√©s",
            "review", "check code", "inspect", "k√≥d ellen≈ërz√©s"
        ]
        
        if any(pattern in query_lower for pattern in code_analysis_patterns):
            # Try to extract file path from query
            import re
            file_path = None
            
            # Look for file extensions
            file_match = re.search(r'([^\\\/\s]+\.(py|js|ts|java|cpp|c|cs|php|rb|go|rs))', query, re.IGNORECASE)
            if file_match:
                file_path = file_match.group(1)
            else:
                # Look for quoted paths
                path_match = re.search(r'["\']([^"\']+\.(py|js|ts|java|cpp|c|cs|php|rb|go|rs))["\']', query, re.IGNORECASE)
                if path_match:
                    file_path = path_match.group(1)
            
            if file_path:
                logger.info(f"üíª Code analysis workflow detected - File: {file_path}")
                
                return {
                    "type": "code_analysis",
                    "task_data": {
                        "type": "code_analysis",
                        "path": file_path,
                        "description": query
                    },
                    "path": file_path
                }
        
        # Multi-step project workflows
        project_patterns = [
            "create project", "build", "generate", "setup",
            "projekt l√©trehoz√°s", "√©p√≠tsd fel", "√°ll√≠tsd √∂ssze"
        ]
        
        if any(pattern in query_lower for pattern in project_patterns):
            if any(tech in query_lower for tech in ["web", "website", "html", "react", "vue", "angular"]):
                logger.info(f"üåê Multi-step web project workflow detected")
                
                return {
                    "type": "multi_step",
                    "task_data": {
                        "type": "multi_step",
                        "description": query,
                        "steps": [
                            {
                                "type": "FILE",
                                "command": {"action": "write", "path": "index.html"},
                                "description": "Create main HTML file"
                            },
                            {
                                "type": "FILE", 
                                "command": {"action": "write", "path": "style.css"},
                                "description": "Create CSS file"
                            },
                            {
                                "type": "FILE",
                                "command": {"action": "write", "path": "script.js"},
                                "description": "Create JavaScript file"
                            }
                        ]
                    }
        }
        
        # No workflow detected
        return None

    async def execute_task_with_core_system(self,
                                           query: str,
                                           system_message: Optional[str] = None,
                                           model: Optional[str] = None,
                                           task_type: Optional[str] = None,
                                           temperature: Optional[float] = None) -> Dict[str, Any]:
        """
        üî• KRITIKUS JAV√çT√ÅS: V√©grehajtja a feladatot az eredeti core_old rendszerrel.
        Ez a met√≥dus haszn√°lja a core_old execution bridge-et a VAL√ìDI tool v√©grehajt√°shoz.
        PLUS: Intelligent workflow integr√°ci√≥ a komplex feladatokhoz.
        """
        start_time = time.time()
        
        # üöÄ NEW: Workflow detection for complex multi-step tasks
        workflow_detected = self._detect_workflow_task(query)
        if workflow_detected:
            try:
                logger.info(f"üîÑ Complex workflow detected, routing to WORKFLOW command: {workflow_detected['type']}")
                
                # Import command router to execute workflow
                from core.command_router import router
                
                # Create WORKFLOW command structure
                workflow_command = {
                    "type": "WORKFLOW",
                    "workflow_type": workflow_detected["type"],
                    "task": workflow_detected["task_data"],
                    "path": workflow_detected.get("path"),
                    "organization_type": workflow_detected.get("organization_type", "by_type"),
                    "remove_duplicates": workflow_detected.get("remove_duplicates", True)
                }
                  # Execute workflow through command router
                workflow_result = await router.route_command(workflow_command)
                execution_time = time.time() - start_time
                
                if workflow_result.get("status") == "success":
                    return {
                        "status": "success",
                        "command_type": "WORKFLOW",
                        "execution_result": workflow_result,
                        "ai_summary": f"‚úÖ Multi-step workflow executed: {workflow_detected['type']}",
                        "execution_type": "MULTI_STEP_WORKFLOW",
                        "response_time": execution_time,
                        "model_used": model or "workflow_system"
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è Workflow execution failed: {workflow_result.get('error', 'Unknown error')}")
                    # Continue with traditional processing
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Workflow processing failed: {e}")
                # Continue with traditional processing
        
        # üöÄ √öJ: Intelligent workflow ellen≈ërz√©s (fallback)
        if INTELLIGENT_WORKFLOWS_AVAILABLE:
            try:
                workflow_result = await process_with_intelligent_workflow(query)
                if workflow_result.get("workflow_detected"):
                    # Ha intelligent workflow-t detekt√°ltunk, haszn√°ljuk azt
                    logger.info(f"üéØ Intelligent workflow haszn√°lata: {query}")
                    execution_time = time.time() - start_time
                    
                    # Intelligent workflow eredm√©ny form√°z√°sa
                    if workflow_result.get("success"):
                        return {
                            "status": "success",
                            "command_type": "INTELLIGENT_WORKFLOW", 
                            "execution_result": workflow_result,
                            "ai_summary": f"‚úÖ Intelligent workflow sikeresen v√©grehajtva: {workflow_result.get('workflow_type', 'unknown')}",
                            "execution_type": "INTELLIGENT_WORKFLOW",
                            "response_time": execution_time,
                            "model_used": model or "intelligent_workflow_system"
                        }
                    else:                        # Ha a workflow hib√°zott, folytassuk a hagyom√°nyos feldolgoz√°ssal
                        logger.warning(f"‚ö†Ô∏è Intelligent workflow failed: {workflow_result.get('error', 'Unknown error')}")
                        
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Intelligent workflow processing failed: {e}")
                # Continue with traditional processing
        
        try:
            # 1. F√ÅZIS: AI elemz√©s √©s parancs felismer√©s
            # üî• KRITIKUS JAV√çT√ÅS: Literal shell parancsok felismer√©se
            # Ha a query egy ismert shell parancs (dir, ls, cat, echo, stb.), ne elemezz√ºk AI-val
            literal_shell_commands = ['dir', 'ls', 'cat', 'echo', 'cd', 'pwd', 'cp', 'mv', 'rm', 'mkdir', 'rmdir', 'find', 'grep', 'ps', 'top', 'ping', 'curl', 'wget', 'git']
            query_first_word = query.strip().split()[0].lower() if query.strip() else ""
            
            if query_first_word in literal_shell_commands:
                # Literal shell parancs - ne elemezz√ºk AI-val, hanem v√©grehajt√°s direktben
                logger.info(f"üîß Literal shell parancs felismerve: {query}")
                command_type = "CMD"
                command_action = query  # A teljes parancsot megtartjuk eredeti form√°ban
                parameters = {}
                ai_response = f"COMMAND_TYPE: CMD\nCOMMAND_ACTION: {query}\nPARAMETERS: {{}}"
            else:
                # Hagyom√°nyos AI elemz√©s √∂sszetett feladatok eset√©n
                analysis_prompt = f"""
                Elemezd ezt a feladatot √©s hat√°rozd meg, milyen konkr√©t parancsokat kell v√©grehajtani:
                
                FELADAT: {query}
                
                El√©rhet≈ë parancs t√≠pusok:
                - ASK: AI k√©rd√©s (pl. 'explain something', 'analyze this')
                - CMD: Shell parancs (pl. 'list files', 'run command') - FONTOS: Ha shell parancs, akkor PONTOSAN azt √≠rd ami a feladatban van!
                - FILE: F√°jl m≈±veletek (pl. 'read file', 'write file', 'create file')
                - CODE: K√≥d m≈±veletek (pl. 'generate code', 'execute python')
                
                V√°laszolj ebben a form√°tumban:
                COMMAND_TYPE: [ASK/CMD/FILE/CODE]
                COMMAND_ACTION: [specific action - ha CMD, akkor PONTOSAN a shell parancsot √≠rd le]
                PARAMETERS: [specific parameters needed]
                """
                if model is None:
                    model = await self.select_model_for_task(query, task_type or "planning")
                analysis_result = await self.ai_client.generate_response(
                    prompt=analysis_prompt,
                    system_message="Te egy parancs elemz≈ë vagy. Legy√©l specifikus √©s pontos. Ha shell parancsot l√°tsz, akkor PONTOSAN azt add vissza mint COMMAND_ACTION!",
                    model=model,
                    task_type="planning"
                )
                # Handle both string and dict responses
                if isinstance(analysis_result, dict):
                    ai_response = analysis_result.get("content", "")
                else:
                    ai_response = str(analysis_result)
                logger.info(f"AI parancs elemz√©s: {ai_response[:200]}...")
                
            # 2. F√ÅZIS: Parancs t√≠pus √©s param√©terek kinyer√©se            
            command_type = "ASK"  # alap√©rtelmezett
            command_action = query
            parameters = {}
            lines = ai_response.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('COMMAND_TYPE:'):
                    command_type = line.replace('COMMAND_TYPE:', '').strip()
                elif line.startswith('COMMAND_ACTION:'):
                    command_action = line.replace('COMMAND_ACTION:', '').strip()
                elif line.startswith('PARAMETERS:'):
                    params_text = line.replace('PARAMETERS:', '').strip()
                    # --- √öJ: JSON √©s egyszer≈± param√©ter felismer√©se ---
                    try:
                        # Pr√≥b√°ljuk JSON-k√©nt √©rtelmezni
                        if params_text.startswith('{') and params_text.endswith('}'):
                            import json
                            parameters = json.loads(params_text)
                        else:
                            # Egyszer≈± string feldolgoz√°s
                            # Speci√°lis kezel√©s a "filename = xyz.txt" form√°tumra
                            if 'filename' in params_text.lower() and '=' in params_text:
                                # Extract filename after "filename = "
                                filename_part = params_text.split('=', 1)[1].strip()
                                if filename_part.endswith(('.txt', '.md', '.log', '.py', '.json', '.html', '.css', '.js')):
                                    parameters['path'] = filename_part
                            elif 'content' in params_text.lower() and '=' in params_text:
                                # Extract content after "content = "
                                content_part = params_text.split('=', 1)[1].strip()
                                parameters['content'] = content_part
                            else:
                                # Ha t√∂bb f√°jln√©v √©s/vagy tartalom is van vessz≈ëvel elv√°lasztva
                                parts = [p.strip() for p in params_text.split(',') if p.strip()]
                                file_names = [p for p in parts if p.endswith('.txt') or p.endswith('.md') or p.endswith('.log')]
                                other_parts = [p for p in parts if not (p.endswith('.txt') or p.endswith('.md') or p.endswith('.log'))]
                                if len(file_names) == 1:
                                    parameters['path'] = file_names[0]
                                elif len(file_names) >= 2:
                                    parameters['path'] = file_names[0]
                                    parameters['target_path'] = file_names[1]
                                # Ha van explicit content
                                for p in other_parts:
                                    if p.lower().startswith('content='):
                                        parameters['content'] = p[len('content='):].strip()
                                    elif p:
                                        parameters['content'] = p  # fallback: b√°rmilyen sz√∂veg
                                if not parameters and params_text:
                                    parameters['data'] = params_text
                    except json.JSONDecodeError:
                        # Ha a JSON parsing nem siker√ºl, visszat√©r√ºnk az egyszer≈± feldolgoz√°sra
                        parts = [p.strip() for p in params_text.split(',') if p.strip()]
                        if params_text:
                            parameters['data'] = params_text
            logger.info(f"√âszlelt parancs: {command_type}, akci√≥: {command_action}, param√©terek: {parameters}")
            # 3. F√ÅZIS: V√©grehajt√°s a core_old rendszerrel
            execution_result = None
            if command_type.upper() == "CMD":
                cmd = command_action if command_action else "echo Hello from Project-S"
                execution_result = await core_execution_bridge.execute_shell_command(cmd)
            elif command_type.upper() == "FILE":
                # F√°jl m≈±velet v√©grehajt√°s
                # Param√©terek normaliz√°l√°sa                # --- JAV√çTOTT LOGIKA ---
                action = "write"
                path = None
                content = None
                # Ha a PARAMETERS csak egy f√°jlnevet tartalmaz
                if 'path' in parameters:
                    path = parameters['path']
                elif 'data' in parameters and isinstance(parameters['data'], str) and parameters['data'].endswith('.txt'):
                    path = parameters['data']
                else:
                    # Intelligens f√°jln√©v kinyer√©s a query-b√≥l
                    path = self._extract_filename_from_query(query)
                content = parameters.get('content', f"Project-S output: {query}")
                # --- ACTION LOGIKA ---
                if "create" in command_action.lower():
                    action = "write"  # 'create file' helyett 'write' kell ide!
                elif "read" in command_action.lower() or "olvas" in query.lower():
                    action = "read"
                    content = None
                # Mindig legyen action √©s path
                file_command = {"action": action, "path": path}
                if content is not None:
                    file_command["content"] = content
                execution_result = await core_execution_bridge.execute_file_operation(file_command)
                if isinstance(execution_result, str):
                    execution_result = {"status": "success", "content": execution_result, "path": path}
            elif command_type.upper() == "CODE":
                action = "generate"
                if "execute" in command_action.lower() or "run" in command_action.lower():
                    action = "execute"
                execution_result = await core_execution_bridge.execute_code_operation(
                    action=action,
                    description=command_action
                )
            else:
                ai_query = command_action if command_action else query
                execution_result = await core_execution_bridge.ask_ai(ai_query)
            execution_time = time.time() - start_time
            if execution_result and isinstance(execution_result, dict) and execution_result.get("status") == "success":
                # --- AI magyar√°z√≥ sz√∂veg hozz√°ad√°sa az eredm√©nyhez ---
                ai_summary = None
                try:
                    ai_summary = await self.ai_client.generate_response(
                        prompt=f"A k√∂vetkez≈ë feladatot sikeresen v√©grehajtottad: {query}\n\n√çrd le r√∂viden, hogy mit csin√°lt a rendszer, √©s mi lett az eredm√©ny!",
                        system_message="Feladat v√©grehajt√°si √∂sszefoglal√≥. L√©gy r√∂vid, informat√≠v, magyarul v√°laszolj!",
                        model=model,
                        task_type="summary"
                    )
                except Exception as e:
                    ai_summary = f"(Nem siker√ºlt AI √∂sszefoglal√≥t gener√°lni: {e})"
                return {
                    "status": "success",
                    "command_type": command_type,
                    "command_action": command_action,
                    "ai_analysis": ai_response[:200] + "..." if len(ai_response) > 200 else ai_response,
                    "execution_result": execution_result,
                    "ai_summary": ai_summary if isinstance(ai_summary, str) else getattr(ai_summary, 'content', str(ai_summary)),
                    "execution_type": "CORE_OLD_SYSTEM",
                    "response_time": execution_time,
                    "model_used": model
                }
            else:
                fallback_result = await self.execute_task_with_model(
                    query=query,
                    system_message=system_message,
                    model=model,
                    task_type=task_type,
                    temperature=temperature
                )
                # Robust error extraction
                if isinstance(execution_result, dict):
                    exec_error = execution_result.get("error")
                else:
                    exec_error = str(execution_result)
                return {
                    "status": "fallback",
                    "command_type": command_type,
                    "ai_analysis": ai_response[:200] + "..." if len(ai_response) > 200 else ai_response,
                    "execution_error": exec_error,
                    "fallback_response": fallback_result,
                    "execution_type": "AI_FALLBACK",
                    "response_time": execution_time,
                    "model_used": model
                }
        except Exception as e:
            logger.error(f"Error in core system execution: {e}")
            execution_time = time.time() - start_time
            try:
                fallback_result = await self.execute_task_with_model(
                    query=query,
                    system_message=system_message,
                    model=model,
                    task_type=task_type,
                    temperature=temperature
                )
                return {
                    "status": "error_fallback",
                    "error": str(e),
                    "fallback_response": fallback_result,
                    "execution_type": "ERROR_FALLBACK",
                    "response_time": execution_time,
                    "model_used": model
                }
            except Exception as fallback_error:
                return {
                    "status": "complete_failure",
                    "error": str(e),
                    "fallback_error": str(fallback_error),
                    "execution_type": "COMPLETE_FAILURE",
                    "response_time": execution_time
                }
    
    def _get_default_model_from_config(self) -> str:
        """
        Bet√∂lti az alap√©rtelmezett modellt a konfigur√°ci√≥b√≥l.
        
        Returns:
            str: Az alap√©rtelmezett modell azonos√≠t√≥ja
        """
        try:
            # Try to get default model from ai_client config
            if hasattr(self.ai_client, 'config') and self.ai_client.config:
                default_model = self.ai_client.config.get("default_model", "qwen3-235b")
                logger.info(f"Alap√©rtelmezett modell konfigur√°ci√≥b√≥l bet√∂ltve: {default_model}")
                return default_model
        except Exception as e:
            logger.warning(f"Nem siker√ºlt bet√∂lteni az alap√©rtelmezett modellt a konfigur√°ci√≥b√≥l: {e}")
        
        # Fallback to qwen3-235b (intended primary model)
        logger.info("Alap√©rtelmezett modell: qwen3-235b (fallback)")
        return "qwen3-235b"
    
    def _extract_filename_from_query(self, query: str) -> str:
        """
        Intelligens f√°jln√©v kinyer√©s a felhaszn√°l√≥i k√©rd√©sb≈ël.
        
        Args:
            query (str): A felhaszn√°l√≥i lek√©rdez√©s
            
        Returns:
            str: A kinyert f√°jln√©v vagy alap√©rtelmezett "project_s_output.txt"
        """
        import re
        
        # 1. K√∂zvetlen√ºl megadott f√°jlnevek keres√©se (id√©z≈ëjelek k√∂z√∂tt vagy kiterjeszt√©ssel)
        # Keres√©s id√©z≈ëjelek k√∂z√∂tt: "filename.txt", 'filename.py'
        quoted_files = re.findall(r'["\']([^"\']+\.[a-zA-Z0-9]+)["\']', query)
        if quoted_files:
            return quoted_files[0]
        
        # 2. Kiterjeszt√©ssel rendelkez≈ë szavak keres√©se
        # Pattern: sz√≥.kiterjeszt√©s (pl. "test.txt", "main.py", "config.json")
        extension_files = re.findall(r'\b([a-zA-Z0-9_-]+\.[a-zA-Z0-9]+)\b', query)
        if extension_files:
            # Kiz√°rjuk a domain neveket √©s URL-eket
            filtered_files = [f for f in extension_files if not f.startswith(('www.', 'http', 'https'))]
            if filtered_files:
                return filtered_files[0]
        
        # 3. F√°jln√©v kulcsszavak alapj√°n
        filename_patterns = {
            # Magyar kulcsszavak
            r'\b(?:hozz\s*l√©tre|k√©sz√≠ts|√≠rj)\s+(?:egy\s+)?([a-zA-Z0-9_-]+)\s+(?:f√°jlt|file)': r'\1.txt',
            r'\b(?:nev≈±|n√©ven)\s+([a-zA-Z0-9_-]+)(?:\s+f√°jl)?': r'\1.txt',
            r'\b([a-zA-Z0-9_-]+)\s+(?:nev≈±|n√©ven)\s+f√°jl': r'\1.txt',
            # Angol kulcsszavak
            r'\b(?:create|make|write)\s+(?:a\s+)?(?:file\s+)?(?:called\s+|named\s+)?([a-zA-Z0-9_-]+)': r'\1.txt',
            r'\b(?:file\s+)?(?:called\s+|named\s+)([a-zA-Z0-9_-]+)': r'\1.txt',
            r'\b([a-zA-Z0-9_-]+)\s+(?:file|f√°jl)': r'\1.txt',
        }
        
        for pattern, replacement in filename_patterns.items():
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                filename = re.sub(pattern, replacement, matches[0], flags=re.IGNORECASE)
                # Tiszt√≠tjuk a f√°jlnevet
                clean_filename = re.sub(r'[^\w\.-]', '_', filename)
                if clean_filename and not clean_filename.startswith('.'):
                    return clean_filename
        
        # 4. Speci√°lis esetek - gyakori f√°jlt√≠pusok felismer√©se
        content_type_mapping = {
            r'\b(?:html|webpage|weboldal)': 'index.html',
            r'\b(?:css|stylesheet|st√≠lus)': 'style.css',
            r'\b(?:js|javascript)': 'script.js',
            r'\b(?:py|python)': 'main.py',
            r'\b(?:json|config|konfig)': 'config.json',
            r'\b(?:md|markdown|readme)': 'README.md',
            r'\b(?:txt|text|sz√∂veg)': 'document.txt',
            r'\b(?:log|napl√≥)': 'log.txt',
            r'\b(?:csv|t√°bl√°zat)': 'data.csv',
        }
        
        for pattern, filename in content_type_mapping.items():
            if re.search(pattern, query, re.IGNORECASE):
                return filename
        
        # 5. Utols√≥ es√©ly: kulcsszavak alapj√°n gener√°l√°s
        if any(word in query.lower() for word in ['list', 'lista', 'jegyz√©k']):
            return 'lista.txt'
        elif any(word in query.lower() for word in ['note', 'jegyzet', 'memo']):
            return 'jegyzet.txt'
        elif any(word in query.lower() for word in ['test', 'teszt', 'pr√≥ba']):
            return 'test.txt'
        elif any(word in query.lower() for word in ['hello', 'hell√≥', '√ºdv√∂zlet']):
            return 'hello.txt'
          # 6. Alap√©rtelmezett visszat√©r√©s
        logger.info(f"Nem siker√ºlt kinyerni a f√°jlnevet a query-b√≥l: '{query}', alap√©rtelmezett haszn√°lata")
        return "project_s_output.txt"
    
    async def process_user_command(self, user_input: str) -> dict:
        """
        Process user command - main entry point for CLI commands.
        Routes to the appropriate execution method based on command type.
        """
        try:
            logger.info(f"Processing user command: {user_input}")
            
            # Use the core system for execution
            result = await self.execute_task_with_core_system(
                query=user_input,
                model="qwen3-235b"  # Default model
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing user command: {e}")
            return {
                "status": "error",
                "error": str(e),
                "command": user_input
            }
    
# Singleton p√©ld√°ny l√©trehoz√°sa
model_manager = ModelManager()
